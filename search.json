[{"path":"https://dajmcdon.github.io/sparsegl/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 2, June 1991Copyright © 1989, 1991 Free Software Foundation, Inc.,51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"licenses software designed take away freedom share change . contrast, GNU General Public License intended guarantee freedom share change free software–make sure software free users. General Public License applies Free Software Foundation’s software program whose authors commit using . (Free Software Foundation software covered GNU Lesser General Public License instead.) can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge service wish), receive source code can get want , can change software use pieces new free programs; know can things. protect rights, need make restrictions forbid anyone deny rights ask surrender rights. restrictions translate certain responsibilities distribute copies software, modify . example, distribute copies program, whether gratis fee, must give recipients rights . must make sure , , receive can get source code. must show terms know rights. protect rights two steps: (1) copyright software, (2) offer license gives legal permission copy, distribute /modify software. Also, author’s protection , want make certain everyone understands warranty free software. software modified someone else passed , want recipients know original, problems introduced others reflect original authors’ reputations. Finally, free program threatened constantly software patents. wish avoid danger redistributors free program individually obtain patent licenses, effect making program proprietary. prevent , made clear patent must licensed everyone’s free use licensed . precise terms conditions copying, distribution modification follow.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/LICENSE.html","id":"terms-and-conditions-for-copying-distribution-and-modification","dir":"","previous_headings":"","what":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION","title":"GNU General Public License","text":"0. License applies program work contains notice placed copyright holder saying may distributed terms General Public License. “Program”, , refers program work, “work based Program” means either Program derivative work copyright law: say, work containing Program portion , either verbatim modifications /translated another language. (Hereinafter, translation included without limitation term “modification”.) licensee addressed “”. Activities copying, distribution modification covered License; outside scope. act running Program restricted, output Program covered contents constitute work based Program (independent made running Program). Whether true depends Program . 1. may copy distribute verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice disclaimer warranty; keep intact notices refer License absence warranty; give recipients Program copy License along Program. may charge fee physical act transferring copy, may option offer warranty protection exchange fee. 2. may modify copy copies Program portion , thus forming work based Program, copy distribute modifications work terms Section 1 , provided also meet conditions: ) must cause modified files carry prominent notices stating changed files date change. b) must cause work distribute publish, whole part contains derived Program part thereof, licensed whole charge third parties terms License. c) modified program normally reads commands interactively run, must cause , started running interactive use ordinary way, print display announcement including appropriate copyright notice notice warranty (else, saying provide warranty) users may redistribute program conditions, telling user view copy License. (Exception: Program interactive normally print announcement, work based Program required print announcement.) requirements apply modified work whole. identifiable sections work derived Program, can reasonably considered independent separate works , License, terms, apply sections distribute separate works. distribute sections part whole work based Program, distribution whole must terms License, whose permissions licensees extend entire whole, thus every part regardless wrote . Thus, intent section claim rights contest rights work written entirely ; rather, intent exercise right control distribution derivative collective works based Program. addition, mere aggregation another work based Program Program (work based Program) volume storage distribution medium bring work scope License. 3. may copy distribute Program (work based , Section 2) object code executable form terms Sections 1 2 provided also one following: ) Accompany complete corresponding machine-readable source code, must distributed terms Sections 1 2 medium customarily used software interchange; , b) Accompany written offer, valid least three years, give third party, charge cost physically performing source distribution, complete machine-readable copy corresponding source code, distributed terms Sections 1 2 medium customarily used software interchange; , c) Accompany information received offer distribute corresponding source code. (alternative allowed noncommercial distribution received program object code executable form offer, accord Subsection b .) source code work means preferred form work making modifications . executable work, complete source code means source code modules contains, plus associated interface definition files, plus scripts used control compilation installation executable. However, special exception, source code distributed need include anything normally distributed (either source binary form) major components (compiler, kernel, ) operating system executable runs, unless component accompanies executable. distribution executable object code made offering access copy designated place, offering equivalent access copy source code place counts distribution source code, even though third parties compelled copy source along object code. 4. may copy, modify, sublicense, distribute Program except expressly provided License. attempt otherwise copy, modify, sublicense distribute Program void, automatically terminate rights License. However, parties received copies, rights, License licenses terminated long parties remain full compliance. 5. required accept License, since signed . However, nothing else grants permission modify distribute Program derivative works. actions prohibited law accept License. Therefore, modifying distributing Program (work based Program), indicate acceptance License , terms conditions copying, distributing modifying Program works based . 6. time redistribute Program (work based Program), recipient automatically receives license original licensor copy, distribute modify Program subject terms conditions. may impose restrictions recipients’ exercise rights granted herein. responsible enforcing compliance third parties License. 7. , consequence court judgment allegation patent infringement reason (limited patent issues), conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. distribute satisfy simultaneously obligations License pertinent obligations, consequence may distribute Program . example, patent license permit royalty-free redistribution Program receive copies directly indirectly , way satisfy License refrain entirely distribution Program. portion section held invalid unenforceable particular circumstance, balance section intended apply section whole intended apply circumstances. purpose section induce infringe patents property right claims contest validity claims; section sole purpose protecting integrity free software distribution system, implemented public license practices. Many people made generous contributions wide range software distributed system reliance consistent application system; author/donor decide willing distribute software system licensee impose choice. section intended make thoroughly clear believed consequence rest License. 8. distribution /use Program restricted certain countries either patents copyrighted interfaces, original copyright holder places Program License may add explicit geographical distribution limitation excluding countries, distribution permitted among countries thus excluded. case, License incorporates limitation written body License. 9. Free Software Foundation may publish revised /new versions General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies version number License applies “later version”, option following terms conditions either version later version published Free Software Foundation. Program specify version number License, may choose version ever published Free Software Foundation. 10. wish incorporate parts Program free programs whose distribution conditions different, write author ask permission. software copyrighted Free Software Foundation, write Free Software Foundation; sometimes make exceptions . decision guided two goals preserving free status derivatives free software promoting sharing reuse software generally.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/LICENSE.html","id":"no-warranty","dir":"","previous_headings":"","what":"NO WARRANTY","title":"GNU General Public License","text":"11. PROGRAM LICENSED FREE CHARGE, WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION. 12. EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MAY MODIFY /REDISTRIBUTE PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES. END TERMS CONDITIONS","code":""},{"path":"https://dajmcdon.github.io/sparsegl/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively convey exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program interactive, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, commands use may called something show w show c; even mouse-clicks menu items–whatever suits program. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. sample; alter names: General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA. Gnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.  <signature of Ty Coon>, 1 April 1989 Ty Coon, President of Vice"},{"path":"https://dajmcdon.github.io/sparsegl/articles/sparsegl.html","id":"installing","dir":"Articles","previous_headings":"","what":"Installing","title":"Getting started with `sparsegl`","text":"package CRAN yet, can installed using [devtools] (https://cran.r-project.org/package=devtools) package: Building vignettes, getting started guide, takes significant amount time. included package default. want include vignettes, use modified command: getting-started vignette, firstly, randomly generate X, input matrix predictors dimension n-obs p-feature. initiate y, real-valued vector (make vector rather matrix), Linear Regression model: \\(y = X\\beta^* + \\epsilon\\). Logistic regression model: \\(y = (y_1, y_2, \\cdots, y_n)\\), \\(y_i \\sim \\text{Bernoulli}\\left(\\frac{1}{1 + \\exp(-X_i \\beta^*)}\\right)\\), \\(= 1, 2, \\cdots, n.\\) coefficient vector \\(\\beta^*\\) specified , white noise \\(\\epsilon\\) following standard normal distribution serves data variation. sparse group-lasso problem formulated sum mean squared error ( linear regression) logistic loss (logistic regression) convex combination lasso penalty group lasso penalty: Linear regression: \\[ \\min_{\\beta\\\\mathbb{R}^p}\\left(\\frac{1}{2n} \\rVert y - \\sum_g X^{(g)}\\beta^{(g)}\\rVert_2^2 + (1-\\alpha)\\lambda\\sum_g \\sqrt{\\text{bs}}\\rVert\\beta^{(g)}\\rVert_2 + \\alpha\\lambda\\rVert\\beta\\rVert_1 \\right) \\qquad (*). \\] Logistic regression: \\[ \\min_{\\beta\\\\mathbb{R}^p}\\left(\\frac{1}{2n}\\sum_{=1}^n \\log\\left(1 + \\exp\\left(-y_i\\beta^\\top X_i\\right)\\right) + (1-\\alpha)\\lambda\\sum_g \\sqrt{\\text{bs}}\\rVert\\beta^{(g)}\\rVert_2 + \\alpha\\lambda\\rVert\\beta\\rVert_1 \\right) \\qquad (**). \\] \\(X^{(g)}\\) submatrix \\(X\\) columns corresponding features group \\(g\\). \\(\\beta^{(g)}\\) corresponding coefficients features group \\(g\\). bs length \\(\\beta^{(g)}\\). \\(\\alpha\\) adjusts weight lasso penalty group-lasso penalty. \\(\\lambda\\) fine-tunes size penalty imposed model control number nonzero coefficients features, furthermore, avoid overfitting.","code":"devtools::install_github(\"dajmcdon/sparsegl\", ref = \"main\") devtools::install_github(\"dajmcdon/sparsegl\", ref = \"main\",                           build_vignettes = TRUE, dependencies = TRUE) library(sparsegl) set.seed(1010) n <- 100 p <- 200 X <- matrix(data = rnorm(n*p, mean = 0, sd = 1), nrow = n, ncol = p) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), c(2, -3, 8, 0, 0), rep(0, (p - 20))) groups <- rep(1:(p / 5), each = 5)  # Linear regression model eps <- rnorm(n, mean = 0, sd = 1) y <- X %*% beta_star + eps  # Logistic regression model pr <- 1 / (1 + exp(-X %*% beta_star)) y_binary <- rbinom(n, 1, pr)"},{"path":"https://dajmcdon.github.io/sparsegl/articles/sparsegl.html","id":"sparsegl","dir":"Articles","previous_headings":"","what":"sparsegl()","title":"Getting started with `sparsegl`","text":"Given input matrix X, response vector y (matrix 1 column), sparse group-lasso regularized linear model fitted sequence penalty parameter values terms penalized maximum likelihood. penalty composed lasso penalty group lasso penalty. main arguments users might give : group: vector consecutive integers length p indicating grouping features. default, group contains one feature without initialization. family: character string specifying likelihood use, either linear regression “gaussian” logistic regression loss “binomial”. Default “gaussian”. pf: penalty vector length number groups total. Default value entry square-root corresponding size group, \\(\\sqrt{\\text{bs}}\\) \\((*)\\) \\((**)\\) . asparse: changes weight lasso penalty, referring \\(\\alpha\\) \\((*)\\) \\((**)\\) : asparse = \\(1\\) gives lasso penalty . asparse = \\(0\\) gives group lasso penalty . default value asparse \\(0.05\\). lower_bnd: lower bound coefficient values, vector length 1 number groups including non-positive numbers . Default value entry -\\(\\infty\\). upper_bnd: upper bound coefficient values, vector length 1 number groups including non-negative numbers . Default value entry \\(\\infty\\). returns sparsegl object, main attribute object : df: number nonzero coefficients lambda value. represents approximation exact degree--freedom. Detailed explanation can checked function estimate_risk() .","code":"fit1 <- sparsegl(X, y, group = groups)"},{"path":"https://dajmcdon.github.io/sparsegl/articles/sparsegl.html","id":"plotting-function-plot-for-sparsegl-object","dir":"Articles","previous_headings":"sparsegl()","what":"Plotting function plot() for sparsegl object","title":"Getting started with `sparsegl`","text":"function produces nonzero-coefficient curves penalty parameter lambda values regularization path fitted sparsegl object. arguments function : y_axis: can set either “coef” “group”. Default “coef”. x_axis: can set either “lambda” “penalty”. Default “lambda”. elaborate y_axis x_axis: plot y_axis = “group” group norms log-lambda scaled group norm vector. group norm defined : \\[ \\text{asparse}\\times\\rVert\\beta^{(g)}\\rVert_1 + (1 - \\text{asparse})\\times\\sum_g\\rVert\\beta^{(g)}\\rVert_2 \\] Curves plotted color corresponding features group. Notice number curves shown plots less actual number groups since groups containing nonzero features least one lambda sequence lambda included. plot y_axis = “coef” coefficients features log-lambda scaled group norm vector. curve distinct color represents feature. , features nonzero coefficients least one lambda value sequence lambda considered. plot x_axis = “lambda” indicates x_axis presenting log-lambda. plot x_axis = “penalty” indicates x_axis presenting scaled group norm vector. element vector defined : \\[ \\frac{\\text{asparse}\\times\\rVert \\beta\\rVert_1 + (1-\\text{asparse})\\times\\sum_g\\rVert \\beta^{(g)}\\rVert_2}{\\max_\\beta\\left(\\text{asparse}\\times\\ \\rVert \\beta\\rVert_1 + (1-\\text{asparse})\\times\\sum_g\\rVert \\beta^{(g)}\\rVert_2\\right)} \\]","code":"plot(fit1, y_axis = \"group\", x_axis = \"lambda\") plot(fit1, y_axis = \"coef\", x_axis = \"penalty\", add_legend = FALSE)"},{"path":"https://dajmcdon.github.io/sparsegl/articles/sparsegl.html","id":"coef-predict-and-print-sparsegl-object","dir":"Articles","previous_headings":"sparsegl()","what":"coef(), predict() and print() sparsegl object","title":"Getting started with `sparsegl`","text":"three functions consume fitted sparsegl object arguments coef() predict() return matrix coefficients predictions \\(\\hat{y}\\) given matrix X lambda respectively, unless optional argument s assigned specified vector numeric value (necessarily) original lambda sequence. print() returns number nonzero features nonzero coefficients lambda.","code":"coef <- coef(fit1, s = c(0.02, 0.03)) pred <- predict(fit1, newx = X[100,], s = fit1$lambda[2:3]) print(fit1)"},{"path":"https://dajmcdon.github.io/sparsegl/articles/sparsegl.html","id":"cv-sparsegl","dir":"Articles","previous_headings":"","what":"cv.sparsegl()","title":"Getting started with `sparsegl`","text":"function k-fold cross-validation (cv) sparsegl. takes arguments X, y, group, specified , additional argument pred.loss. can set either “L2” “L1” linear regression model, “loss” “misclass” logistic regression model indicating loss use cv error. return cv.sparseg object.","code":"fit_l1 <- cv.sparsegl(X, y, group = groups, pred.loss = \"L1\")"},{"path":"https://dajmcdon.github.io/sparsegl/articles/sparsegl.html","id":"plot-coef-and-predict-for-cv-sparsegl-object","dir":"Articles","previous_headings":"cv.sparsegl()","what":"plot(), coef() and predict() for cv.sparsegl object","title":"Getting started with `sparsegl`","text":"plot(): produces cross-validation curve upper lower confidence bounds plots lambda regularization path fitted cv.sparsegl object. generated plot, red points represent cvm values lambda, attached gray vertical error bars indicate uncertainty cvm.  coef() predict() cv.sparsegl object work similarly introduced . differences argument object passes fitted cv.sparsegl object, s can also set lambda.1se, lambda.min addition.","code":"plot(fit_l1) coef <- coef(fit_l1, s = \"lambda.1se\") pred <- predict(fit_l1, newx = X[50:80, ], s = \"lambda.min\")"},{"path":"https://dajmcdon.github.io/sparsegl/articles/sparsegl.html","id":"estimate_risk","dir":"Articles","previous_headings":"","what":"estimate_risk()","title":"Getting started with `sparsegl`","text":"function returns information criterion, sum maximum log-likelihood penalty term determined chosen penalty type sparsegl model lambda. provides means model selection representing trade-goodness fit model complexity model. takes arguments X y function sparsegl(). additional arguments needs : object: fitted sparsegl object. type: three types penalty used calculation: AIC (Akaike information criterion): 2 * df / n BIC (Bayesian information criterion): log(n) * df / n GCV (Generalized cross validation): -2 * log(1 - df / n) df degree--freedom, n sample size. approx_df: indicate vector approximation degree--freedom penalty parameter lambda used. Default FALSE program compute unbiased estimate exact degree--freedom. FYI: Degree--freedom tool assess complexity statistical modeling procedure. object$df, approximation degree--freedom number nonzero coefficients model. Notice take time calculate unbiased estimate exact degreeo--freedom X complicated. details realize calculation, method implemented based paper https://arxiv.org/pdf/1212.6478.pdf.","code":"risk <- estimate_risk(fit1, X, y, type = \"AIC\", approx_df = FALSE)"},{"path":"https://dajmcdon.github.io/sparsegl/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Daniel J. McDonald. Author, maintainer. Xiaoxuan Liang. Author. Aaron Cohen. Author.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"McDonald D, Liang X, Cohen (2022). sparsegl: Sparse Group Lasso. R package version 0.3.0, https://github.com/dajmcdon/sparsegl.","code":"@Manual{,   title = {sparsegl: Sparse Group Lasso},   author = {Daniel J. McDonald and Xiaoxuan Liang and Aaron Cohen},   year = {2022},   note = {R package version 0.3.0},   url = {https://github.com/dajmcdon/sparsegl}, }"},{"path":"https://dajmcdon.github.io/sparsegl/index.html","id":"r-package-sparsegl","dir":"","previous_headings":"","what":"Sparse Group Lasso","title":"Sparse Group Lasso","text":"goal sparsegl fit regularization paths sparse group-lasso penalized learning problems. model fit sequence regularization parameter lambda.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Sparse Group Lasso","text":"can install development version Github :","code":"# install.packages(\"remotes\") remotes::install_github(\"dajmcdon/sparsegl\")"},{"path":"https://dajmcdon.github.io/sparsegl/index.html","id":"minimal-example","dir":"","previous_headings":"","what":"Minimal Example","title":"Sparse Group Lasso","text":"","code":"set.seed(1010) n <- 100 p <- 200 X <- matrix(data = rnorm(n*p, mean = 0, sd = 1), nrow = n, ncol = p) eps <- rnorm(n, mean = 0, sd = 1) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0),                 rep(-5, 5), c(2, -3, 8, 0, 0), rep(0, (p - 20))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) fit1 <- sparsegl(X, y, group = groups) plot(fit1, y_axis = \"coef\", x_axis = \"penalty\", add_legend = FALSE)"},{"path":"https://dajmcdon.github.io/sparsegl/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Sparse Group Lasso","text":"package documentation examples available online.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.cv.sparsegl.html","id":null,"dir":"Reference","previous_headings":"","what":"Get coefficients from a cv.sparsegl object. — coef.cv.sparsegl","title":"Get coefficients from a cv.sparsegl object. — coef.cv.sparsegl","text":"function gets coefficients cross-validated sparsegl() model, using stored \"sparsegl.fit\" object, optimal value chosen lambda.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.cv.sparsegl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get coefficients from a cv.sparsegl object. — coef.cv.sparsegl","text":"","code":"# S3 method for cv.sparsegl coef(object, s = c(\"lambda.1se\", \"lambda.min\"), ...)"},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.cv.sparsegl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get coefficients from a cv.sparsegl object. — coef.cv.sparsegl","text":"object Fitted cv.sparsegl() object. s Value(s) penalty parameter lambda coefficients desired. Default single value s = \"lambda.1se\" stored CV object (corresponding largest value lambda CV error estimate within 1 standard error minimum). Alternatively s = \"lambda.min\" can used (corresponding minimum cross validation error estimate). s numeric, taken value(s) lambda used. ... used. arguments predict().","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.cv.sparsegl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get coefficients from a cv.sparsegl object. — coef.cv.sparsegl","text":"coefficients requested value(s) lambda.","code":""},{"path":[]},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.cv.sparsegl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get coefficients from a cv.sparsegl object. — coef.cv.sparsegl","text":"","code":"n <- 100 p <- 20 X <- matrix(rnorm(n * p), nrow = n) eps <- rnorm(n) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) fit1 <- sparsegl(X, y, group = groups) cv_fit <- cv.sparsegl(X, y, groups) coef(cv_fit, s = c(0.02, 0.03)) #> 21 x 2 sparse Matrix of class \"dgCMatrix\" #>                       1           2 #> (Intercept) -0.03154183 -0.07708669 #> V1           4.54448507  4.36458528 #> V2           4.67735712  4.54595959 #> V3           4.68155454  4.55672652 #> V4           4.64448718  4.49005342 #> V5           4.80942560  4.70582975 #> V6           4.70845282  4.53868199 #> V7          -4.54914749 -4.38812591 #> V8           1.82027569  1.77795075 #> V9          -0.28132170 -0.34475311 #> V10          0.27855223  0.34059250 #> V11         -4.60595902 -4.42717702 #> V12         -4.81777045 -4.70867587 #> V13         -4.77442143 -4.68888125 #> V14         -4.83862354 -4.72264756 #> V15         -4.54925658 -4.39345015 #> V16          .           .          #> V17          .           .          #> V18          .           .          #> V19          .           .          #> V20          .           ."},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.sparsegl.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract model coefficients from a sparsegl object. — coef.sparsegl","title":"Extract model coefficients from a sparsegl object. — coef.sparsegl","text":"Computes coefficients requested value(s) lambda sparsegl() object.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.sparsegl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract model coefficients from a sparsegl object. — coef.sparsegl","text":"","code":"# S3 method for sparsegl coef(object, s = NULL, ...)"},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.sparsegl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract model coefficients from a sparsegl object. — coef.sparsegl","text":"object Fitted sparsegl() object. s Value(s) penalty parameter lambda coefficients required. Default entire sequence. ... used.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.sparsegl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract model coefficients from a sparsegl object. — coef.sparsegl","text":"coefficients requested values lambda.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.sparsegl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract model coefficients from a sparsegl object. — coef.sparsegl","text":"s new vector predictions requested. s lambda sequence used fitting model, coef function use linear interpolation make predictions. new values interpolated using fraction coefficients left right lambda indices.","code":""},{"path":[]},{"path":"https://dajmcdon.github.io/sparsegl/reference/coef.sparsegl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract model coefficients from a sparsegl object. — coef.sparsegl","text":"","code":"n <- 100 p <- 20 X <- matrix(rnorm(n * p), nrow = n) eps <- rnorm(n) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) fit1 <- sparsegl(X, y, group = groups) coef(fit1, s = c(0.02, 0.03)) #> 21 x 2 sparse Matrix of class \"dgCMatrix\" #>                       1           2 #> (Intercept)  0.06731442  0.14379532 #> V1           5.10994114  5.04549248 #> V2           4.98615624  4.95747418 #> V3           4.72448341  4.62639506 #> V4           4.78824199  4.63355177 #> V5           4.55167994  4.35248173 #> V6           4.69345110  4.56140784 #> V7          -4.60470796 -4.41971396 #> V8           1.80726224  1.74754923 #> V9          -0.19218806 -0.24176197 #> V10         -0.06283659 -0.05277343 #> V11         -4.53477451 -4.39786968 #> V12         -4.78754081 -4.65430437 #> V13         -4.77275353 -4.63389003 #> V14         -4.78116905 -4.68941755 #> V15         -4.85159814 -4.67827748 #> V16          .           .          #> V17          .           .          #> V18          .           .          #> V19          .           .          #> V20          .           ."},{"path":"https://dajmcdon.github.io/sparsegl/reference/cv.sparsegl.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation for a sparsegl object. — cv.sparsegl","title":"Cross-validation for a sparsegl object. — cv.sparsegl","text":"k-fold cross-validation sparsegl(). function largely similar glmnet::cv.glmnet().","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/cv.sparsegl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation for a sparsegl object. — cv.sparsegl","text":"","code":"cv.sparsegl(   x,   y,   group = NULL,   family = c(\"gaussian\", \"binomial\"),   lambda = NULL,   pred.loss = c(\"L2\", \"L1\", \"binomial\", \"misclass\"),   nfolds = 10,   foldid = NULL,   ... )"},{"path":"https://dajmcdon.github.io/sparsegl/reference/cv.sparsegl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation for a sparsegl object. — cv.sparsegl","text":"x Double. matrix predictors, dimension \\(n \\times p\\); row vector measurements column feature. Objects class Matrix::sparseMatrix supported. y Double/Integer/Factor. response variable. Quantitative family=\"gaussian\". family=\"binomial\" either factor two levels vector integers taking 2 unique values. factor, last level alphabetical order target class. group Integer. vector consecutive integers describing grouping coefficients (see example ). family Character. Specifies loss function use, valid options : \"gaussian\" - least squares loss (regression, default), \"binomial\" - logistic loss (classification) lambda user supplied lambda sequence. default, NULL results automatic computation based nlambda, smallest value lambda give null model (coefficient estimates equal zero), lambda.factor. Supplying value lambda overrides behaviour. likely better supply decreasing sequence lambda values single (small) value. supplied, user-defined lambda sequence automatically sorted decreasing order. pred.loss Loss use cross-validation error. Valid options : \"L2\" regression, mean square error \"L1\" regression, mean absolute error \"binomial\" classification, binomial deviance loss \"misclass\" classification, misclassification error. nfolds Number folds - default 10. Although nfolds can large sample size (leave-one-CV), recommended large datasets. Smallest value allowable nfolds = 3. foldid optional vector values 1 nfolds identifying fold observation . supplied, nfolds can missing. ... arguments can passed sparsegl.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/cv.sparsegl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation for a sparsegl object. — cv.sparsegl","text":"object class cv.sparsegl() returned, list ingredients cross-validation fit. lambda values lambda used fits. cvm mean cross-validated error - vector length length(lambda). cvsd Estimate standard error cvm. cvupper Upper curve = cvm + cvsd. cvlower Lower curve = cvm - cvsd. name text string indicating type measure (plotting purposes). sparsegl.fit fitted sparsegl() object full data. lambda.min optimal value lambda gives minimum cross validation error cvm. lambda.1se largest value lambda error within 1 standard error minimum.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/cv.sparsegl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-validation for a sparsegl object. — cv.sparsegl","text":"function runs sparsegl() nfolds + 1 times; first get lambda sequence, remainder compute fit folds omitted. average error standard deviation folds computed.","code":""},{"path":[]},{"path":"https://dajmcdon.github.io/sparsegl/reference/cv.sparsegl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validation for a sparsegl object. — cv.sparsegl","text":"","code":"n <- 100 p <- 20 X <- matrix(rnorm(n * p), nrow = n) eps <- rnorm(n) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) cv_fit <- cv.sparsegl(X, y, groups)"},{"path":"https://dajmcdon.github.io/sparsegl/reference/estimate_risk.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate information criteria. — estimate_risk","title":"Calculate information criteria. — estimate_risk","text":"function uses degrees freedom calculate various information criteria. function uses \"unknown variance\" version likelihood. implemented Gaussian regression. constant ignored (stats::extractAIC()).","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/estimate_risk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate information criteria. — estimate_risk","text":"","code":"estimate_risk(object, x, y, type = c(\"AIC\", \"BIC\", \"GCV\"), approx_df = FALSE)"},{"path":"https://dajmcdon.github.io/sparsegl/reference/estimate_risk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate information criteria. — estimate_risk","text":"object fitted object call sparsegl(). x Double. matrix predictors, dimension \\(n \\times p\\); row vector measurements column feature. Objects class Matrix::sparseMatrix supported. y Double/Integer/Factor. response variable. Quantitative family=\"gaussian\". family=\"binomial\" either factor two levels vector integers taking 2 unique values. factor, last level alphabetical order target class. type one AIC, BIC, GCV. approx_df df component sparsegl() object approximation (albeit fairly accurate one) actual degrees--freedom. However, exact value requires inverting portion X'X. computation may take time (default computes exact df).","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/estimate_risk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate information criteria. — estimate_risk","text":"data.frame many rows object$lambda. contains columns lambda, df, requested risk types.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/estimate_risk.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate information criteria. — estimate_risk","text":"Vaiter S, Deledalle C, Peyré G, Fadili J, Dossal C. (2012). Degrees Freedom Group Lasso General Design. https://arxiv.org/pdf/1212.6478.pdf.","code":""},{"path":[]},{"path":"https://dajmcdon.github.io/sparsegl/reference/estimate_risk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate information criteria. — estimate_risk","text":"","code":"n <- 100 p <- 20 X <- matrix(rnorm(n * p), nrow = n) eps <- rnorm(n) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) fit1 <- sparsegl(X, y, group = groups) estimate_risk(fit1, X, y, type = \"AIC\") #>           lambda        df       AIC #> s0  5.123292e-01  0.000000 5.5465936 #> s1  4.668153e-01  3.769496 5.6210618 #> s2  4.253447e-01  4.982464 5.5611280 #> s3  3.875583e-01  9.924416 5.5633395 #> s4  3.531286e-01  9.979292 5.4309164 #> s5  3.217577e-01  9.987910 5.3027498 #> s6  2.931736e-01  9.991636 5.1802573 #> s7  2.671289e-01 14.973660 5.1117065 #> s8  2.433979e-01 13.988521 4.9155734 #> s9  2.217751e-01 13.991977 4.7385924 #> s10 2.020732e-01 13.993875 4.5611446 #> s11 1.841216e-01 13.995107 4.3834422 #> s12 1.677647e-01 13.995980 4.2056786 #> s13 1.528610e-01 13.996633 4.0280442 #> s14 1.392812e-01 13.997140 3.8507353 #> s15 1.269079e-01 13.997545 3.6739609 #> s16 1.156337e-01 13.997874 3.4979485 #> s17 1.053611e-01 13.998147 3.3229501 #> s18 9.600114e-02 13.998376 3.1492469 #> s19 8.747266e-02 13.998569 2.9771552 #> s20 7.970183e-02 13.998735 2.8070304 #> s21 7.262134e-02 13.998877 2.6392712 #> s22 6.616986e-02 13.999001 2.4743221 #> s23 6.029151e-02 13.999109 2.3126742 #> s24 5.493538e-02 14.999098 2.1747193 #> s25 5.005507e-02 14.999193 2.0211008 #> s26 4.560832e-02 14.999276 1.8724580 #> s27 4.155660e-02 14.999349 1.7294139 #> s28 3.786483e-02 14.999414 1.5926709 #> s29 3.450102e-02 14.999472 1.4626646 #> s30 3.143604e-02 14.999524 1.3400266 #> s31 2.864335e-02 14.999570 1.2252298 #> s32 2.609876e-02 14.999612 1.1186438 #> s33 2.378021e-02 13.999689 1.0005916 #> s34 2.166765e-02 13.999719 0.9111033 #> s35 1.974275e-02 13.999745 0.8301301 #> s36 1.798886e-02 13.999769 0.7574817 #> s37 1.639078e-02 13.999791 0.6929258 #> s38 1.493467e-02 13.999810 0.6358435 #> s39 1.360791e-02 14.999805 0.6057639 #> s40 1.239902e-02 14.999823 0.5620860 #> s41 1.129753e-02 14.999840 0.5242961 #> s42 1.029389e-02 14.999854 0.4917846 #> s43 9.379407e-03 14.999868 0.4639544 #> s44 8.546166e-03 14.999880 0.4402369 #> s45 7.786948e-03 14.999891 0.4201658 #> s46 7.095177e-03 14.999901 0.4031223 #> s47 6.464861e-03 14.999910 0.3887435 #> s48 5.890541e-03 14.999918 0.3766437 #> s49 5.367241e-03 14.999925 0.3664834 #> s50 4.890430e-03 14.999932 0.3579672 #> s51 4.455978e-03 14.999938 0.3508399 #> s52 4.060121e-03 14.999944 0.3448828 #> s53 3.699431e-03 19.969481 0.4387074 #> s54 3.370784e-03 19.986585 0.4344512 #> s55 3.071333e-03 18.993588 0.4107093 #> s56 2.798484e-03 18.995462 0.4075080 #> s57 2.549875e-03 18.996570 0.4048298 #> s58 2.323351e-03 18.997297 0.4025938 #> s59 2.116951e-03 19.997131 0.4207336 #> s60 1.928887e-03 19.997628 0.4191740 #> s61 1.757530e-03 19.998008 0.4178729 #> s62 1.601396e-03 19.998307 0.4167888 #> s63 1.459132e-03 19.998547 0.4158857 #> s64 1.329507e-03 19.998742 0.4151338 #> s65 1.211397e-03 19.998905 0.4145080 #> s66 1.103780e-03 19.999041 0.4139873 #> s67 1.005723e-03 19.999156 0.4135541 #> s68 9.163774e-04 19.999253 0.4132047 #> s69 8.349689e-04 19.999338 0.4129041 #> s70 7.607926e-04 19.999410 0.4126737 #> s71 6.932059e-04 19.999474 0.4124651 #> s72 6.316234e-04 19.999530 0.4122891 #> s73 5.755117e-04 19.999579 0.4121424 #> s74 5.243848e-04 19.999623 0.4120204 #> s75 4.777999e-04 19.999661 0.4119190 #> s76 4.353535e-04 19.999695 0.4118348 #> s77 3.966779e-04 19.999726 0.4117649 #> s78 3.614381e-04 19.999752 0.4117211 #> s79 3.293290e-04 19.999777 0.4116606 #> s80 3.000723e-04 19.999798 0.4116292 #> s81 2.734147e-04 19.999817 0.4115974 #> s82 2.491253e-04 19.999834 0.4115690 #> s83 2.269937e-04 19.999850 0.4115446 #> s84 2.068282e-04 19.999864 0.4115242 #> s85 1.884541e-04 19.999877 0.4115071 #> s86 1.717124e-04 19.999888 0.4114929 #> s87 1.564579e-04 19.999899 0.4114811 #> s88 1.425586e-04 19.999908 0.4114712 #> s89 1.298941e-04 19.999917 0.4114631 #> s90 1.183547e-04 19.999924 0.4114564 #> s91 1.078404e-04 19.999931 0.4114508 #> s92 9.826014e-05 19.999938 0.4114461 #> s93 8.953098e-05 19.999943 0.4114423 #> s94 8.157730e-05 19.999948 0.4114391 #> s95 7.433019e-05 19.999953 0.4114365 #> s96 6.772691e-05 19.999957 0.4114343 #> s97 6.171023e-05 19.999961 0.4114325 #> s98 5.622807e-05 19.999965 0.4114310 #> s99 5.123292e-05 19.999968 0.4114298"},{"path":"https://dajmcdon.github.io/sparsegl/reference/grouped_sp_norm.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate common group norms — one_norm","title":"Calculate common group norms — one_norm","text":"Norm calculation","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/grouped_sp_norm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate common group norms — one_norm","text":"","code":"one_norm(x)  two_norm(x)  grouped_one_norm(x, gr)  grouped_two_norm(x, gr)  grouped_sp_norm(x, gr, asparse)  gr_one_norm(x, gr)  gr_two_norm(x, gr)  sp_group_norm(x, gr, asparse = 0.05)"},{"path":"https://dajmcdon.github.io/sparsegl/reference/grouped_sp_norm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate common group norms — one_norm","text":"x numeric vector. gr numeric vector length x. asparse weight put l1 norm calculating group norm.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/grouped_sp_norm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate common group norms — one_norm","text":"numeric scalar vector","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/grouped_sp_norm.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Calculate common group norms — one_norm","text":"one_norm: l1-norm (Absolute-value norm). two_norm: l2-norm (Euclidean norm). grouped_one_norm: vector group-wise l1-norms. grouped_two_norm: vector group-wise l2-norms. grouped_sp_norm: vector length unique(gr) consisting asparse convex combination l1 l2-norm group. gr_one_norm: l1-norm norm vector (scalar). gr_two_norm: sum group-wise l2-norms vector (scalar). sp_group_norm: sum asparse convex combination group l1 l2-norms vectors (scalar).","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/grouped_sp_norm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate common group norms — one_norm","text":"","code":"x <- c(rep(-1, 5), rep(0, 5), rep(1,5)) gr <- c(rep(1,5), rep(2,5), rep(3,5)) asparse <- 0.05 grouped_sp_norm(x, gr, asparse) #> [1] 2.374265 0.000000 2.374265"},{"path":"https://dajmcdon.github.io/sparsegl/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://dajmcdon.github.io/sparsegl/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/plot.cv.sparsegl.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot cross-validation curves produced from a cv.sparsegl object. — plot.cv.sparsegl","title":"Plot cross-validation curves produced from a cv.sparsegl object. — plot.cv.sparsegl","text":"Plots cross-validation curve, upper lower standard deviation curves, function lambda values used. function modified based glmnet::plot.cv.glmnet().","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/plot.cv.sparsegl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot cross-validation curves produced from a cv.sparsegl object. — plot.cv.sparsegl","text":"","code":"# S3 method for cv.sparsegl plot(x, log_axis = c(\"xy\", \"x\", \"y\", \"none\"), sign.lambda = 1, ...)"},{"path":"https://dajmcdon.github.io/sparsegl/reference/plot.cv.sparsegl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot cross-validation curves produced from a cv.sparsegl object. — plot.cv.sparsegl","text":"x Fitted cv.sparsegl() object log_axis Apply log scaling requested axes. sign.lambda Either plot log(lambda) (default) reverse sign.lambda < 0. ... used.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/plot.cv.sparsegl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot cross-validation curves produced from a cv.sparsegl object. — plot.cv.sparsegl","text":"plot produced, ggplot2::ggplot() object. Additional user modifications can added desired.","code":""},{"path":[]},{"path":"https://dajmcdon.github.io/sparsegl/reference/plot.cv.sparsegl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot cross-validation curves produced from a cv.sparsegl object. — plot.cv.sparsegl","text":"","code":"n <- 100 p <- 20 X <- matrix(rnorm(n * p), nrow = n) eps <- rnorm(n) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) cv_fit <- cv.sparsegl(X, y, groups) plot(cv_fit)"},{"path":"https://dajmcdon.github.io/sparsegl/reference/plot.sparsegl.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot solution paths from a sparsegl object. — plot.sparsegl","title":"Plot solution paths from a sparsegl object. — plot.sparsegl","text":"Produces coefficient profile plot fitted sparsegl() object. result ggplot2::ggplot(). Additional user modifications can added desired.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/plot.sparsegl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot solution paths from a sparsegl object. — plot.sparsegl","text":"","code":"# S3 method for sparsegl plot(   x,   y_axis = c(\"coef\", \"group\"),   x_axis = c(\"lambda\", \"penalty\"),   add_legend = TRUE,   ... )"},{"path":"https://dajmcdon.github.io/sparsegl/reference/plot.sparsegl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot solution paths from a sparsegl object. — plot.sparsegl","text":"x Fitted sparsegl() object. y_axis Variable y_axis. Either coefficients (default) group norm. x_axis Variable x-axis. Either (log)-lambda sequence (default) value penalty. second case, penalty scaled maximum along path. add_legend Show legend. Often, many groups/predictors, can become overwhelming. ... used.","code":""},{"path":[]},{"path":"https://dajmcdon.github.io/sparsegl/reference/plot.sparsegl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot solution paths from a sparsegl object. — plot.sparsegl","text":"","code":"n <- 100 p <- 20 X <- matrix(rnorm(n * p), nrow = n) eps <- rnorm(n) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) fit1 <- sparsegl(X, y, group = groups) plot(fit1, y_axis = \"coef\", x_axis = \"penalty\")"},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.cv.sparsegl.html","id":null,"dir":"Reference","previous_headings":"","what":"Make predictions from a cv.sparsegl object. — predict.cv.sparsegl","title":"Make predictions from a cv.sparsegl object. — predict.cv.sparsegl","text":"function makes predictions cross-validated cv.sparsegl() object, using stored \"sparsegl.fit\" object, value chosen lambda.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.cv.sparsegl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make predictions from a cv.sparsegl object. — predict.cv.sparsegl","text":"","code":"# S3 method for cv.sparsegl predict(object, newx, s = c(\"lambda.1se\", \"lambda.min\"), ...)"},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.cv.sparsegl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make predictions from a cv.sparsegl object. — predict.cv.sparsegl","text":"object Fitted cv.sparsegl() object. newx Matrix new values x predictions made. Must matrix. See documentation predict.sparsegl(). s Value(s) penalty parameter lambda coefficients desired. Default single value s = \"lambda.1se\" stored CV object (corresponding largest value lambda CV error estimate within 1 standard error minimum). Alternatively s = \"lambda.min\" can used (corresponding minimum cross validation error estimate). s numeric, taken value(s) lambda used. ... used. arguments predict().","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.cv.sparsegl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make predictions from a cv.sparsegl object. — predict.cv.sparsegl","text":"matrix vector predicted values.","code":""},{"path":[]},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.cv.sparsegl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make predictions from a cv.sparsegl object. — predict.cv.sparsegl","text":"","code":"n <- 100 p <- 20 X <- matrix(rnorm(n * p), nrow = n) eps <- rnorm(n) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) fit1 <- sparsegl(X, y, group = groups) cv_fit <- cv.sparsegl(X, y, groups) predict(cv_fit, newx = X[50:60, ], s = \"lambda.min\") #>                1 #>  [1,]  29.218183 #>  [2,]  26.004006 #>  [3,]  28.685502 #>  [4,] -26.995122 #>  [5,]   8.491413 #>  [6,] -30.413055 #>  [7,] -25.512583 #>  [8,]  -6.381012 #>  [9,]   6.052066 #> [10,]   6.023908 #> [11,] -19.796916"},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.sparsegl.html","id":null,"dir":"Reference","previous_headings":"","what":"Make predictions from a sparsegl object. — predict.sparsegl","title":"Make predictions from a sparsegl object. — predict.sparsegl","text":"Similar predict methods, function produces fitted values class labels fitted sparsegl object.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.sparsegl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make predictions from a sparsegl object. — predict.sparsegl","text":"","code":"# S3 method for sparsegl predict(   object,   newx,   s = NULL,   type = c(\"link\", \"response\", \"coefficients\", \"nonzero\", \"class\"),   ... )"},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.sparsegl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make predictions from a sparsegl object. — predict.sparsegl","text":"object Fitted sparsegl() model object. newx Matrix new values x predictions made. Must matrix. s Value(s) penalty parameter lambda predictions required. Default entire sequence used create model. type Type prediction required. Type \"link\" gives linear predictors \"binomial\"; \"gaussian\" models gives fitted values. Type \"response\" gives fitted probabilities \"binomial\"; \"gaussian\" type \"response\" equivalent type \"link\". Type \"coefficients\" computes coefficients requested values s.  Note \"binomial\" models, results returned class corresponding second level factor response. Type \"class\" applies \"binomial\" models, produces class label corresponding maximum probability. Type \"nonzero\" returns list indices nonzero coefficients value s. ... used.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.sparsegl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make predictions from a sparsegl object. — predict.sparsegl","text":"object returned depends type.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.sparsegl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make predictions from a sparsegl object. — predict.sparsegl","text":"s new vector predictions requested. s lambda sequence used fitting model, function use linear interpolation make predictions. new values interpolated using fraction predicted values left right lambda indices.","code":""},{"path":[]},{"path":"https://dajmcdon.github.io/sparsegl/reference/predict.sparsegl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make predictions from a sparsegl object. — predict.sparsegl","text":"","code":"n <- 100 p <- 20 X <- matrix(rnorm(n * p), nrow = n) eps <- rnorm(n) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) fit1 <- sparsegl(X, y, group = groups) predict(fit1, newx = X[10, ], s = fit1$lambda[3:5]) #>              1        2         3 #> [1,] -4.036048 -5.10366 -6.046824"},{"path":"https://dajmcdon.github.io/sparsegl/reference/print.sparsegl.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a sparsegl object. — print.sparsegl","title":"Print a sparsegl object. — print.sparsegl","text":"Prints summary information fitted sparsegl() object.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/print.sparsegl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a sparsegl object. — print.sparsegl","text":"","code":"# S3 method for sparsegl print(x, digits = min(3, getOption(\"digits\") - 3), ...)"},{"path":"https://dajmcdon.github.io/sparsegl/reference/print.sparsegl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a sparsegl object. — print.sparsegl","text":"x Fitted sparsegl() object. digits Significant digits printout. ... used","code":""},{"path":[]},{"path":"https://dajmcdon.github.io/sparsegl/reference/print.sparsegl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print a sparsegl object. — print.sparsegl","text":"","code":"n <- 100 p <- 20 X <- matrix(rnorm(n * p), nrow = n) eps <- rnorm(n) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) fit1 <- sparsegl(X, y, group = groups) print(fit1) #>  #> Call:  sparsegl(x = X, y = y, group = groups)  #>  #> Approx. degrees of freedom:  0  -  20  #> Range of lambda:  0.73  -  0  #> Saturated penalty:  31.72"},{"path":"https://dajmcdon.github.io/sparsegl/reference/sparsegl.html","id":null,"dir":"Reference","previous_headings":"","what":"Fits the regularization paths for sparse group-lasso penalized learning problems. — sparsegl","title":"Fits the regularization paths for sparse group-lasso penalized learning problems. — sparsegl","text":"Fits regularization paths sparse group-lasso penalized learning problems sequence regularization parameters lambda.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/sparsegl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fits the regularization paths for sparse group-lasso penalized learning problems. — sparsegl","text":"","code":"sparsegl(   x,   y,   group = NULL,   family = c(\"gaussian\", \"binomial\"),   nlambda = 100,   lambda.factor = ifelse(nobs < nvars, 0.01, 1e-04),   lambda = NULL,   pf = sqrt(bs),   intercept = TRUE,   asparse = 0.05,   standardize = TRUE,   lower_bnd = -Inf,   upper_bnd = Inf,   dfmax = as.integer(max(group)) + 1L,   pmax = min(dfmax * 1.2, as.integer(max(group))),   eps = 1e-08,   maxit = 3e+08 )"},{"path":"https://dajmcdon.github.io/sparsegl/reference/sparsegl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fits the regularization paths for sparse group-lasso penalized learning problems. — sparsegl","text":"x Double. matrix predictors, dimension \\(n \\times p\\); row vector measurements column feature. Objects class Matrix::sparseMatrix supported. y Double/Integer/Factor. response variable. Quantitative family=\"gaussian\". family=\"binomial\" either factor two levels vector integers taking 2 unique values. factor, last level alphabetical order target class. group Integer. vector consecutive integers describing grouping coefficients (see example ). family Character. Specifies loss function use, valid options : \"gaussian\" - least squares loss (regression, default), \"binomial\" - logistic loss (classification) nlambda number lambda values - default 100. lambda.factor factor getting minimal lambda lambda sequence, min(lambda) = lambda.factor * max(lambda). max(lambda) smallest value lambda coefficients zero. default depends relationship \\(n\\) (number rows matrix predictors) \\(p\\) (number predictors). \\(n \\geq p\\), default 0.0001.  \\(n < p\\), default 0.01. small value lambda.factor lead saturated fit. argument effect user-defined lambda sequence. lambda user supplied lambda sequence. default, NULL results automatic computation based nlambda, smallest value lambda give null model (coefficient estimates equal zero), lambda.factor. Supplying value lambda overrides behaviour. likely better supply decreasing sequence lambda values single (small) value. supplied, user-defined lambda sequence automatically sorted decreasing order. pf Penalty factor, vector length total number groups. Separate penalty weights can applied group \\(\\beta\\)s allow differential shrinkage. Can 0 groups, implies shrinkage, results group always included model. Default value entry square-root corresponding size group. intercept Whether include intercept model. Default TRUE. asparse weight put \\(\\ell_1\\)-norm sparse group lasso. Default 0.05. standardize Logical flag variable standardization (scaling) prior fitting model. Default TRUE. lower_bnd Lower bound coefficient values, vector length 1 length number groups. Must non-positive numbers . Default value entry -Inf. upper_bnd Upper coefficient values, vector length 1 length number groups. Must non-negative numbers . Default value entry Inf. dfmax Limit maximum number groups model. Default limit. pmax Limit maximum number groups ever nonzero. example group enters model, matter many times exits re-enters model path, counted . eps Convergence termination tolerance. Defaults value 1e-8. maxit Maximum number outer-loop iterations allowed fixed lambda value. Default 3e8. models converge, consider increasing maxit.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/sparsegl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fits the regularization paths for sparse group-lasso penalized learning problems. — sparsegl","text":"object S3 class sparsegl(). call call produced object. b0 Intercept sequence length length(lambda). beta p x length(lambda) sparse matrix coefficients. df number features nonzero coefficients value lambda. dim Dimension coefficient matrix. lambda actual sequence lambda values used. npasses Total number iterations summed lambda values. jerr Error flag, warnings errors, 0 error. group vector consecutive integers describing grouping coefficients.","code":""},{"path":"https://dajmcdon.github.io/sparsegl/reference/sparsegl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fits the regularization paths for sparse group-lasso penalized learning problems. — sparsegl","text":"Note objective function least squares $$RSS/(2n) + \\lambda \\times \\text{penalty}$$ Users can also tweak penalty choosing different penalty factor. computing speed reason, models converging running slowly, consider increasing eps, decreasing nlambda, increasing lambda.factor increasing maxit.","code":""},{"path":[]},{"path":"https://dajmcdon.github.io/sparsegl/reference/sparsegl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fits the regularization paths for sparse group-lasso penalized learning problems. — sparsegl","text":"","code":"n <- 100 p <- 20 X <- matrix(rnorm(n * p), nrow = n) eps <- rnorm(n) beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15))) y <- X %*% beta_star + eps groups <- rep(1:(p / 5), each = 5) fit <- sparsegl(X, y, group = groups)"}]
