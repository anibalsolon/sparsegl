---
documentclass: jss
author:
  - name: Daniel McDonald
    affiliation: |
      | University of British Columbia
    address: |
      | Department of Statistics
      | Vancouver, BC Canada
    email: \email{daniel@stat.ubc.ca}
    url: \url{https://dajmcdon.github.io/}
  - name: Xiaoxuan Liang
    affiliation: 'University of British Columbia \AND'
    address: |
      | Department of Statistics
      | Vancouver, BC Canada
  - name: Aaron Cohen
    affiliation: 'Indiana University'
    address: |
      | Department of Statistics
      | Bloomington, IN USA
title:
  formatted: "Estimating Sparse Group Lasso with the \\pkg{sparsegl} R Package"
  # If you use tex in the formatted title, also supply version without
  plain:     "Sparse Group Lasso with \\pkg{sparsegl}"
  # For running headers, if needed
  short:     "\\pkg{sparsegl}: Sparse Group Lasso"
abstract: >
  The sparse group lasso is a high-dimensional regression technique that is
  useful for problems whose covariates have a naturally grouped structure, and
  where sparsity is encouraged at both the group and individual covariate
  level. In this paper we discuss two algorithms for this optimization
  problem, as well as their implementation in Fortran as part of a new R
  package. This R package is an advance over existing packages, as it is the
  only one that solves this problem in a sufficiently fast computation time
  for truly high-dimensional situations.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
preamble: >
  \usepackage{amsmath}
  \input{jss-sparsegl-preamble}
output: rticles::jss_article
bibliography: [sparsegl.bib, pkgs.bib]
keep_tex: true
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, fig.path = "fig/")
options(prompt = 'R> ', continue = '+ ')
# Add any packages we want to cite to the list below. Don't edit the bib
# manually, or changes will be overwritten. Add other references to
# `sparsegl.bib`
# 
# Note: tags are R-<pkg>
knitr::write_bib(c("devtools", "knitr", "testthat", "usethis",
                   "rticles", "sparsegl", "glmnet",
                   "SGL","gglasso", "msgl", "biglasso",
                   "microbenchmark", "RSpectra",
                   "ggplot2", "dotCall64"),
                 file = "pkgs.bib")
```

# Introduction

\nocite{R-gglasso,R-SGL}

Regularized linear models are now ubiquitous tools for prediction and increasingly inference. When solving the high-dimensional learning problems, adding regularization helps to reduce the chances of overfitting and improve the model performance on unseen data. Sparsity inducing  $\ell_1$-type penalties such as the lasso \citep{tibshirani1996regression} or the Dantzig selector 
\citep{CandesTao2007} perform both variable selection and shrinkage, resulting in near-optimal
statistical properties. The group lasso \citep{yuan2006model} modifies the regularizer with a group $\ell_2$ for grouped covariates (e.g. one-hot factor variables). The resulting estimate will include or exclude entire groups of covariates. To simultaneously attain sparsity at both group and individual feature levels, \citet{simon2013sparse} proposed the sparse group-lasso convex combination of the $\ell_1$ lasso penalty and the group lasso-penalty. 

While a number of packages exist for solving the sparse group lasso, our \proglang{R} implementation in \pkg{sparsegl} is designed to be fast, especially in the case of large, sparse covariate matrices. This package focuses on finding the optimal solutions to the sparse group-lasso penalized learning problems at a sequence of regularization parameters, implements risk estimators in an effort to avoid cross validation if necessary, leverages a fast, compiled \proglang{Fortran} implementation, avoids extraneous data copies, and undertakes a number of additional computational efficiency improvements.

In \proglang{R}, there are already excellent implementations of sparse group lasso and group lasso, namely \pkg{SGL}, \pkg{gglasso}, and \pkg{biglasso}. Only \pkg{SGL} employs the additional $\ell_1$ sparsity-inducing penalty. However, it has a number of drawbacks that result in much slower performance, even on small data. One major reason is the omission of so-called "strong rules" \citep{tibshirani2012strong} that help gradient descent algorithms to ignore many groups. The \pkg{gglasso} and \pkg{biglasso} \citep{zeng2020biglasso} packages are both computationally fast. The former incorporates the strong rule, and the latter involves a hybrid safe-strong rule along with scalable storage and a parallel implementation in \proglang{C++} and \proglang{R} that allows for data that exceeds the size of installed random access memory. Unfortunately, neither allows do not incorporate within-group sparsity (i.e., it performs group lasso, not sparse group lasso). When this package is used on a grouped dataset, the solution will have some active groups and some inactive, but within an active group, generally all the coefficients will be nonzero. 


<!-- \pkg{msgl} \citep{vincent2014sparse} is also an \proglang{R} package solving sparsel group lasso multiclassification problems via fitting multinomial logistic regression model. This package overcomes the nondifferential penalty problem at zero and since the algorithm is Newton-based, it substitutes the Hessian matrix with an upper bound on that, which reduces the costs of computation. However, the multinomial logistic regression is an extension to the binary logistic regression, allowing for taking the response variable with multiple classes, which consumes an excessive amount of time for model fitting when processing binary response variable. -->


In \proglang{python}, \pkg{asgl} \citep{civieta2020adaptive} implements an adaptive sparse group-lasso, which improves the variable selection accuracy by flexibly adjusting the weights in the penalization on the groups of features. As with other packages mentioned above, it can also solve the special cases (lasso and group lasso). Additionally it incorporates quantile loss. For all optimization problems, it directly uses \pkg{cvxpy} without strong rules or other tricks.

<!-- The paper \citep{ida2019fast} also brings up a fast algorithm applying KKT and revised KKT checks repetitively through block coordinate descent for computing sparse group-lasso model. In this algorithm, the main feature is that the revised KKT check first searches through all groups to filter out the groups whose feature coefficients are zero, then the original KKT check confirms the actual nonzero coefficient groups among the remaining candidate active groups. The revised KKT check could remarkably reduce the time complexity, which is introduced by approximating a metric used to compare against a fixed threshold with a tight upper bound.-->

Our contribution, then, is to provide a package that performs sparse group lasso, and is faster than existing implementations. In particular, \pkg{sparsegl} has the following benefits:
\begin{itemize}
\item Performs Gaussian and logistic regression;
\item Allows for interval constraints on the coefficients;
\item Accommodates a sparse design matrix and returns the coefficient estimates in a sparse matrix;
\item Uses strong rules for fast computation along a sequence of tuning parameters;
\item Uses \pkg{dotCall64} to interface with low-level \proglang{Fortran} functions and avoid unnecessary copying as well as allow for 64-bit integers \citep[see][]{gerber2017dotcall,gerber2018dotcall};
\item Provides information criteria as risk estimators (AIC/BIC/GCV);
\end{itemize}
In Section \ref{methodology-estimation-and-prediction}, we describe the algorithmic implementation in detail, paying particular attention to the strong rule. In Section \ref{example-usage}, we show how to use the package, running through an example with simulated data. 
<!-- In Section \ref{comparisons-with-other-packages}, we compare our package to the other existing group-lasso packages, and also compare the two variants of our algorithm with each other. -->
We summarize and discuss paths for future work in Section \ref{discussion-and-future-work}, and make promising proposals for potential future research as well.





# Methodology, estimation and prediction

Given a sample of $n$ observations of a univariate response $y_i$ and $p$ features $\mathbf{x}_i$, the standard linear regression setup has
$$y_i = \mathbf{x}_i^\T \vbeta + \sigma\epsilon_i,\ \ i=1,\ldots,n,$$ where $\epsilon_i$ is independent standard Gaussian noise and $\sigma > 0$. While standard linear regression estimates the coefficient vector $\vbeta$ by solving $\min_{\vbeta} \frac{1}{2n}\sum_{i=1}^n (y_i - \vx_i^\T\vbeta)^2$, this method tends to behave poorly if $p \gg n$. In what follows, we will write $\vy = (y_1,\ldots,y_n)$ and let $\mX$ be the rowwise concatenation of $\vx^\T_1,\ldots,\vx_n^\T$.


The lasso addresses this difficulty by adding an $\ell_1$ penalty to the
optimization problem:
\begin{equation}
\label{eq:lasso}
\min_{\vbeta} \left\{\frac{1}{2n} \sum_{i=1}^n (y_i - \vx_i^\T\vbeta)^2 + \lambda\sum_{j=1}^p |\beta_j|\right\} = \min_{\vbeta}\left\{\frac{1}{2n}\norm{\vY-\mX\vbeta}_2^2 + \lambda \norm{\vbeta}_1\right\},
\end{equation}
where $\norm{\cdot}_2$ is the Euclidean ($\ell_2$) norm and $\norm{\cdot}_1$ is the $\ell_1$ norm.
The benefit of this penalty is that it tends to allow only a subset of coefficient estimates to be nonzero, hence performing variable selection. Here, $\lambda$ is a hyperparameter that trades fidelity to the data---small $\lambda$ emphasizes minimization of the squared-error---with desirable regularization. 

A variant of this, the group lasso \citep{yuan2006model} is appropriate when there is a natural grouping structure for the features. That is, we assume that both the design matrix $\mX$ and the corresponding vector of coefficients can be partitioned into meaningful groups, and, by analogy with lasso regression, only a few of the groups are active, i.e., have nonzero coefficients. The group lasso thus performs regularization that has the effect of discarding groups of predictors rather than the predictors themselves:
\begin{equation}
\min_{\vbeta}\left\{\frac{1}{2n}\big\lVert \vY-\sum_{g=1}^G \mX^{(g)}\vbeta^{(g)}\big\rVert_2^2 + \lambda\sum_{l=1}^m\sqrt{w_g}\snorm{\vbeta^{(g)}}_2\right\}.
\label{eq:group-lasso}
\end{equation}
Note that the grouping structure is explicitly stated: the vector of coefficients, $\vbeta$, is thought of as a concatenation of the coefficient subvectors of the various groups $\vbeta^{(g)}$, and similarly the data matrix $\mX$ is the concatenation of submatrices, each submatrix $\mX^{(g)}$ being composed of the columns that correspond to the particular group. Thus the first part of the equation, $\vY-\sum_{g=1}^G\mX^{(g)}\vbeta^{(g)}$, is identical to the more simply-written equation $\vY-\mX\vbeta$, but the notation serves to emphasise the partitioning.

The penalty however---$\sum_{g=1}^G\sqrt{w_g}\snorm{\vbeta^{(g)}}_2$---is different from the corresponding part in Equation \eqref{eq:lasso}, using instead the sum of the (non-squared) $\ell_2$-norms of the coefficient vectors of the various groups. It is the non-differentiability of this expression at $\mathbf{0}\in\R^{|g|}$ (with $|g|$ meaning the size of group $g$) that accounts for the group-discarding property of the problem, similar to the way that the  non-differentiability the absolute value at $0$ is responsible for discarding individual predictors in the lasso.


As with Equation \eqref{eq:lasso}, there is only a single tuning parameter $\lambda$, whose value determines the strength of regularization. Within the second summation are the relative weights of the groups, $w_g$. These are often taken to be the size of the corresponding group. For simplicity, this notation is suppressed below where the meaning is clear.

Finally, in a group-structured problem as above, it may be desirable to enforce sparsity, not only among the groups, but also within the groups. The sparse group lasso \citep{simon2013sparse} does this by combining the penalties in Equations \eqref{eq:lasso} and \eqref{eq:group-lasso}:
\begin{equation}
  \label{eq:sparsegl}
\min_{\vbeta}\left\{\frac{1}{2}\snorm{\vY-\mX\vbeta}_2^2 + (1-\alpha)\lambda\sum_{g=1}^G \snorm{\vbeta^{(g)}}_2+\alpha\lambda\snorm{\vbeta}_1\right\}.
\end{equation}
There is now a second tuning parameter $\alpha$, which controls the relative emphasis of intra- vs inter-group sparsity in the predictors.

<!--
The solution to the sparse group lasso in Equation \eqref{eq:sparsegl}, like the other two procedures, does not have a closed form expression and thus relies on algorithmic implementations, which may be computationally expensive for large datasets and/or many values of $\lambda$. The rest of this paper therefore concerns the mitigation of this issue, in particular the following question: if we are solving the sparse group lasso, not for a single value of the tuning parameter $\lambda$ but a whole parameter space---in this context, a range of values $\{\lambda_1,\dots \lambda_S\}$--- can we shorten the total computational time by making use of the already-computed solution at previous $\lambda$s' in the parameter space to speed up computation at a given $\lambda$?

In this paper we make use of a heuristic called the Strong Rule \citep{tibshirani2012strong} to help solve this problem, with some success. The strong rule uses the solution at the previous $\lambda_{s-1}$ to predict which groups will remain inactive upon solution at the current $\lambda_s$, and discards those groups before entering into the algorithm. If a significant number of groups are thrown out before entering the algorithm, convergence time can improve significantly. The \texttt{R} package we have created has an algorithm called \textbf{four-step algorithm}, which uses the strong rule. 
-->


## The group-wise majorization-minimization algorithm

There is no closed-form solution to the optimization problem in Equation \eqref{eq:sparsegl}, so we require a numerical procedure. Because the problem is convex, a variety of methods may be used.
The general framework for our algorithm is the same as the majorized block-wise coordinate descent algorithm developed in \citep{yang2015fast, simon2013sparse}. What this means is that, for a fixed value of $\lambda$, we loop over the groups and update only those coefficients while holding all other groups constant. In particular, instead of using the exact Hessian to determine the step size and direction in every update step, we update according to a simpler expression that majorizes the expression we want to solve for. 

For the rest of this section, we describe this majorization algorithm, focusing on a particular group $k$ and holding the coefficients for all other groups fixed. We note here that, because the loss function in Equation \eqref{eq:sparsegl} is differentiable and the penalty terms are convex and separable (i.e., they can be decomposed into a sum of functions each only involving a single group), this block coordinate descent algorithm is guaranteed to converge to a global optimum \citep{tseng2001convergence}.

To begin with, we introduce some notation. Let 
\[
\vR_{(-k)} = \vY - \sum_{g \neq k} \mX^{(g)} \vbeta^{(g)}
\]
be the partial residual without group $k$ where all the group fits besides that of group $k$ are subtracted from $\vY$. With all other groups held fixed, we aim to solve:
\begin{equation}
	\label{eq:sparsegroupk}
\min_{\vbeta^{(k)}} \frac{1}{2n} \snorm{\vR_{(-k)}-\mX^{(k)}\vbeta^{(k)}}_2^2 + (1-\alpha)\lambda \snorm{\vbeta^{(k)}}_2 + \alpha \lambda \snorm{\vbeta^{(k)}}_1. 
\end{equation}
In what follows, we will suppress the $(k)$ notation, with the understanding that we are really referring to only the $k^\textrm{th}$ group of the coefficient vector and the partial residual $\vR_{(-k)}$. We will also define, the unpenalized loss function
\[
\ell (\vbeta) = \frac{1}{2n}\snorm{\vR - \mX\vbeta}_2^2, 
\]
so that our objective function for the $k^\textrm{th}$ group becomes $\ell (\vbeta) + (1-\alpha)\lambda\norm{\vbeta}_2 + \alpha \lambda \norm{\vbeta}_1$, and we are interested in finding an optimal value, $\hat{\vbeta}$. This enables the procedure to generalize easily to logistic loss or, in principle, other exponential families.

Any global minimum must satisfy a subgradient equation, similar to a first-derivative test for a, except that $\norm{\cdot}_2$ and $\norm{\cdot}_1$ are non-differentiable at $\mathbf{0}$. For Equation \eqref{eq:sparsegroupk} above, taking the subdifferential and setting equal to zero gives us the following first-order condition: 
\begin{equation}
  \label{eq:subgrad}
\nabla \ell(\vbeta) = (1-\alpha)\lambda \mathbf{u} + \alpha \lambda \mathbf{v},
\end{equation}
where $\mathbf{u}$ is the subgradient of $\norm{\vbeta}_2$ and $\mathbf{v}$ is the subgradient of $\norm{\vbeta}_1$. The first is defined to be $\vbeta
/ \norm{\vbeta}_2$ if $\vbeta$ is a nonzero vector, and is any vector in the set $\{\mathbf{u} : \norm{\mathbf{u}}_2 \leq 1 \}$ otherwise; the second, $\mathbf{v}$, is defined coordinate-wise as $v_j = \text{sign}(\beta_j)$ if $v_j \neq 0$, and is any value $v_j \in \{v_j : |v_j| \leq 1 \}$ otherwise.


<!--At this point, it is possible to use \autoref{eq:subgrad} to derive an update step and develop a cyclical coordinate-wise algorithm, but it is too computationally expensive, involving repeated large-matrix multiplication (\citep{simon2013sparse}); instead we use the majorization-minimization idea discussed above.-->

For the Gaussian case, the unpenalized loss $\ell (\vbeta)$ is a quadratic function in $\vbeta$, so it is equal to its second order Taylor expansion about any point $\vbeta_0$ in the parameter space. We thus start with the following equality for any given $\vbeta_0$ (recalling that $\vbeta_0$ here is really only for group $k$):  
\begin{equation}
\forall \vbeta,\ \vbeta_0,\quad \ell(\vbeta) = \ell(\vbeta_0)+(\vbeta - \vbeta_0)^\T\nabla \ell(\vbeta_0)+\frac{1}{2}(\vbeta - \vbeta_0)^\T \Hessian (\vbeta - \vbeta_0),
\label{eq:TaylorExp}
\end{equation}
where the gradient $\nabla \ell$ is the first total derivative of $\ell$ (evaluated at $\vbeta_0$) and $\Hessian$, the Hessian, is the second total derivative. For $\ell(\vbeta) = \frac{1}{2n}\snorm{\vR - \mX\vbeta}_2^2$, a short computation shows that the Hessian is $\Hessian = \mX^\T \mX$.

For the large-scale problems motivating this work, the matrix $\mX$ is large, so computing $\mX^\T \mX$, storing it in memory, or inverting it, is computationally prohibitive. Instead, we replace this matrix with a simpler one, $t^{-1}\mathbf{I}$, a diagonal matrix with the value of $t$ selected to be such that this dominates the Hessian (in the sense that $t^{-1}\mathbf{I} - \Hessian$ is positive definite). For our algorithm we choose the largest eigenvalue of the Hessian and use that for $t^{-1}$. Note that this eigenvalue must be computed for each group $g \in G$, but this computation is relatively simple using the power method or other techniques as implemented with \pkg{RSpectra} \citep{R-RSpectra}.
This upper bound leads to the following inequality:
\begin{equation}
\forall \vbeta,\ \vbeta_0,\quad \ell(\vbeta) \leq \ell(\vbeta_0)+(\vbeta - \vbeta_0)^\T\nabla \ell(\vbeta_0)+\frac{1}{2t}(\vbeta - \vbeta_0)^\T  (\vbeta - \vbeta_0).
\label{eq:dominate}
\end{equation}

Replacing the loss function in the original minimization problem in Equation \eqref{eq:sparsegroupk} with the right-hand side of Equation \eqref{eq:dominate}
leads to a majorized version of the original problem
\begin{equation}
\label{eq:Meq}
M(\vbeta) = \ell(\vbeta_0)+(\vbeta - \vbeta_0)^\T\nabla \ell(\vbeta_0)+\frac{1}{2t}\norm{\vbeta_0-\vbeta}_2^2+ (1-\alpha)\lambda\norm{\vbeta}_2+\alpha\lambda\norm{\vbeta}_1,
\end{equation}
which no longer involves operations with the Hessian matrix. 


As before, the optimal value for $M(\vbeta)$ is determined by its subgradient equation,  similar to that of Equation \eqref{eq:subgrad}:
\begin{equation}
\frac{1}{t} (\vbeta - (\vbeta_0 - t\nabla \ell)) +(1-\alpha)\lambda \mathbf{u} + \alpha \lambda \mathbf{v} = \mathbf{0},
\end{equation}
with $\mathbf{u}$ and $\mathbf{v}$ as defined above. Solving this for $\vbeta$ in terms of $\vbeta_0$ results in the following expression:
\begin{equation}
\hat{\vbeta} = U(\vbeta_0) =
\left(1-\frac{t(1-\alpha)\lambda}{\norm{S(\vbeta_0-t\nabla \ell(\vbeta_0),\ t\alpha\lambda)}_2}\right)_+ S(\vbeta_0-t\nabla \ell(\vbeta_0),\ t\alpha\lambda),
\label{eq:updateStep}
\end{equation}
where $(z)_+ = \max\{z,\ 0\}$ and $S$ is the coordinate-wise soft threshold operator, on a vector $\boldsymbol\gamma$ and scalar $b$,
\begin{equation}
\label{softthresh}
(S(\boldsymbol\gamma,b))_j = \text{sign}(\gamma_j)(|\gamma_j| - b)_+,
\end{equation}
i.e., for each coordinate in the vector, it shrinks that coordinate in magnitude by the amount $b$, and sets it to zero if the magnitude of that coordinate was smaller than $b$ to begin with. It is this soft-thresholding operation that encourages within-group sparsity.

An examination Equation \eqref{eq:updateStep} shows that it is possible for the whole group to be set to zero (made inactive) due to the (hard) threshold operator $(\cdot)_+$ in the first part of the expression. It is also possible for individual components of $\vbeta^{(k)}$ to be zeroed out from the coordinate-wise (soft) threshold operator $S(\cdot)$. Therefore, preforming this update step tends to enforce covariate sparsity at both the group and individual level.


<!--\autoref{eq:updateStep} defines the update step at a given group $k$, via $\vbeta \leftarrow U(\vbeta)$. At a point $\vbeta_{old} = \hat{\vbeta}^{(k)}_{old}$, we find the convex function $M$ that majorizes the objective function--centered at that $\vbeta_{old}$--and choose the new value of beta to be the minimizer of majorized function, $\vbeta_{new} = U(\vbeta_{old})$. This update step has the advantage that it is easy to compute, and is guaranteed to converge to the optimum $\hat{\vbeta} = \hat{\vbeta}^{(k)}$ for that given group.  -->

\begin{algorithm}[tb!]
  \caption{Sparse group lasso solution}
  \label{alg:sparsegl}
  \begin{algorithmic}
    \STATE {\bfseries Input:} $\lambda \geq 0$, $\alpha \in [0,\ 1]$, set of groups $\mathcal{G}$, initial coefficients $\vbeta$
    \WHILE{Not converged}
    \FOR{$g \in \mathcal{G}$}
    \STATE Update $\vbeta^{(g)} = \left(1-\frac{t(1-\alpha)\lambda}{\norm{S(\vbeta^{(g)}-t\nabla \ell(\vbeta^{(g)}),\ t\alpha\lambda)}_2}\right)_+ S(\vbeta^{(g)}-t\nabla \ell(\vbeta^{(g)}),\ t\alpha\lambda)$
    \ENDFOR
    \ENDWHILE 
    \RETURN{$\vbeta$}
  \end{algorithmic}
\end{algorithm}


<!-- Both of the algorithms in the \pkg{sparsegl} package use the update step $U$ in \autoref{eq:updateStep}, iterating over all the groups and performing one update per pass. Since the partial residual $\vR_{(-k)}$ is used in $U$ (via $\ell$), we note that, for a given group $k$ at a given point in the cycle, the update $\vbeta^{(k)} \leftarrow U(\vbeta^{(k)})$ uses the new values of $\{\vbeta^{(1)}, \dots \vbeta^{(k-1)}\}$ and the old values for the subsequent $\{\vbeta^{(k+1)},\dots \vbeta^{(m)}\}$. -->

Above, we have focused on the Gaussian linear model with $\ell(\vbeta) = \frac{1}{2n}\snorm{\vR - \mX\vbeta}_2^2$, $\nabla \ell(\vbeta) = -\frac{1}{n}\mX^\T (\vR - \mX\vbeta)$, and $\mathbf{H}\leq t\mathbf{I}$. 
In the case of logistic regression, we use exactly the same procedure but with
$\ell(\vbeta) = \frac{1}{n}\sum_{i}\log(1 + \exp\{-r_i\vx_i^\T\vbeta\})$, $\nabla \ell(\vbeta) = -\frac{1}{n}\sum_{i}y_i\vx_i^\top (1 + \exp\{-r_i\vx_i^\T\vbeta\})^{-1}$, and $\mathbf{H}(\vbeta) \leq 4t^{-1}\mathbf{I}$. This procedure is explicitly stated in \autoref{alg:sparsegl}.

While the procedure described so far solves Equation \eqref{eq:sparsegl} for fixed choices of $\lambda$ and $\alpha$, the data analyst does not typically know these ahead of time. Rather, we would like to solve the problem for a collection of values of $\lambda$ (and perhaps $\alpha$ as well). It turns out that the structure of this optimization problem allows for some heuristics that can perform this sequential optimization with a minimum of additional computational resources, in some cases, solving Equation \eqref{eq:sparsegl} faster for a sequence of values $\lambda_s \in \{\lambda_1,\ldots,\lambda_S\}$ than for an single choice \citep{tibshirani2012strong}. We describe our implementation of this procedure next.


<!-- Because the optimization problem is convex, with differentiable loss and separable penalty, it can be shown that this type of blockwise coordinate descent algorithm is guaranteed to converge to the optimal solution for the full vector $\hat{\vbeta}$. That is, it does not get `stuck' at any inflection points or local minima. Therefore, if that was all the algorithm did, there would be no need to check the KKT conditions for optimality. -->

<!-- However, our algorithms both make use of the sequential strong rule, a heuristic that predicts, before computation, which groups will remain inactive, and discards them; this results in significant computational savings, but because this pre-processing prediction is not always correct, we will need to do a KKT check \citep{boyd2004convex} after convergence to make sure the initial prediction was accurate. This is described in the next section. -->




## KKT Check and the Strong Rule

As mentioned above, the strong rule \citep{tibshirani2012strong} is easy and fast to perform, discarding large numbers of predictors before computation, but it is not guaranteed to be correct, so it becomes necessary to double-check the solution after convergence, to make sure active groups were not accidentally discarded. 

The version of the strong rule used here is called the \textbf{sequential strong rule}; it makes use of the fact that we are solving for a sequence $\{\lambda_1 \geq \lambda_2 \geq \dots \geq\lambda_S\}$ of lambda parameters, rather than a single value. At each $\lambda_s$, we rely on the fact that we have solved the problem for the previous $\lambda_{s-1}$, and use this information to quickly discard many predictors. Without loss of generality, for the rest of this section assume that the problem has already been solved for $\lambda_{s-1}$.



The motivation for the strong rule comes from the KKT stationarity condition, which is similar to the first derivative test for minima \citep{boyd2004convex}. Explicitly, this is the condition that the subdifferential equations (see \autoref{eq:subgrad}) are satisfied at the proposed minimum $\hat{\vbeta} =\hat{\vbeta}(\lambda_s)$. This is the necessary and sufficient condition for a global optimum. Note that we will be emphasizing the dependence of $\hat{\vbeta},\vR_{(-k)}$, and so forth, on $\lambda$ in this section.
 
From \autoref{eq:subgrad} we derive an inequality that is equivalent to the KKT stationarity condition in the case that $\hat{\vbeta}^{(k)} = \mathbf{0}$. Because the algorithm is guaranteed to converge to the optimum for all groups on which it is run, we only need to check the condition for the discarded groups, i.e., those that we have preassigned $\hat{\vbeta}^{(k)} = \mathbf{0}$ (at a given $\lambda$). 

First, if $\hat{\vbeta}^{(k)} = \mathbf{0}$, the subgradient equations reduce to the following:

\begin{equation}
\label{eq:subgradreduced}
\mX^{(k)\T}\vR_{(-k)}/n - \alpha \lambda \mathbf{v} = (1-\alpha) \lambda \mathbf{u},
\end{equation}
where $\mathbf{u}$ and $\mathbf{v}$ are the subdifferentials of $\norm{\vbeta^{(k)}}_2$ and $\norm{\vbeta^{(k)}}_1$, as in section 2.1. We claim that if the following inequality

\begin{equation}
\label{eq:kkt}
\norm{S(\mX^{(k)\T}\vR_{(-k)}/n,\ \alpha\lambda)}_2 \leq (1-\alpha)\lambda
\end{equation}
is satisfied (with $S(\cdot)$ the soft-threshold operator from section 2.1), then $\hat{\vbeta}^{(k)} = \mathbf{0}$ is the optimum for that group at that particular $\lambda$. In other words, this inequality is equivalent to the KKT stationarity condition for inactive groups.

The proof is as follows: denote $\boldsymbol\gamma = \mX^{(k)\T}\vR_{(-k)}/n$, and define $v_j = \text{sign}(\gamma_j)$ if $|\gamma_j| > \alpha \lambda$, and $v_j = \frac{\gamma_j}{\alpha \lambda}$ otherwise. Then 
\[
\boldsymbol\gamma - \alpha \lambda \mathbf{v} = S(\boldsymbol\gamma,\alpha \lambda).
\]
Next, define $\mathbf{u} = S(\boldsymbol\gamma,\alpha \lambda)/(1-\alpha)(\lambda)$. From these two definitions it follows that \autoref{eq:subgradreduced} is satisfied, but not necessarily by elements of the subdifferentials. But if \autoref{eq:kkt} is also satisfied, then $\boldsymbol{u}$ is a subgradient of $\norm{\vbeta^{(k)}}_2$, and $\boldsymbol{v}$ a subgradient of $\norm{\vbeta^{(k)}}_1$, evaluated at $\vbeta^{(k)}=0$.

We have derived the KKT condition for the $k$-th group to be $0$. We will now derive and explain the strong rule, which is based on this inequality. At this point it will be helpful to make explicit the dependence of these expressions on $\lambda$. First, for each group $k$, we define a function of $\lambda$:

\begin{equation}
c_k(\lambda) = S(\mX^{(k)\T}\vR_{(-k)}(\lambda)/n,\ \alpha\lambda).
\end{equation}
So for example \autoref{eq:kkt} becomes $\norm{c_k(\lambda)}_2 \leq (1-\alpha)\lambda$. There is a different function for every group $k$. Note that, since $\vR_{(-k)}$ involves $\hat{\vbeta}$, we needed to have actually solved the optimization problem at $\lambda$ in order to compute $c_k(\lambda)$.

It is natural to consider the properties such a function would have. These functions are continuous, for example, as the solution to the optimization problem, \autoref{eq:sparsegl} varies continuously with its tuning parameter $\lambda$. We are now going to make an assumption about $c_k$, which seems unintuitive at first, and in fact is not always true, but is true often enough to be useful: we assume that $c_k(\lambda)$ is $(1-\alpha)$-Lipschitz, i.e., that
\begin{equation}
\label{eq:lipschitz}
\forall \lambda,\ \lambda^{\prime}\ \  \ \norm{c_k(\lambda)-c_k(\lambda^{\prime})}_2 \leq (1-\alpha)\norm{\lambda - \lambda^{\prime}}_2.
\end{equation}
This is essentially saying that $c_k$ is not only continuous but does not vary 'too fast'; this is equivalent to saying that the function is differentiable almost everywhere and has bounded derivative.


We can finally define the \textbf{strong rule} check for discarding groups. Assume that the optimization problem has been solved for $\lambda_{s-1}$; then if the inequality
\begin{equation}
\label{eq:strong}
\norm{c_k(\lambda_{s-1})}_2 \leq (1-\alpha)(2\lambda_s - \lambda_{s-1})
\end{equation}
holds, then we discard group $k$ from the problem, before solving for $\lambda_s$. That is to say, when we move from $\lambda_{s-1}$ to $\lambda_s$, we first run this check on all groups, using the previously computed solution for $\hat{\vbeta}(\lambda_{s-1})$, and then move on to coordinate descent, using only those groups that failed this inequality.

This discarding rule is fast, using the previously computed solution and a simple inequality, and in practice it tends to accurately discard large numbers of groups. The reason why it works is as follows: assuming the Lipschitz condition on $c_k(\lambda)$, then if the strong rule is satisfied for $\lambda_s$, we get by the triangle inequality that
\begin{align*}
\norm{c_k(\lambda_s)}_2 &\leq \norm{c_k(\lambda_s) - c_k(\lambda_{s-1})}_2 + \norm{c_k(\lambda_{s-1})}_2 \\
&\leq (1-\alpha)(\lambda_{s-1} - \lambda_s) + (1-\alpha)(2\lambda_s - \lambda_{s-1})
=(1-\alpha)\lambda_s,
\end{align*}
which is exactly the KKT condition required for $\hat{\vbeta}^{(k)}(\lambda_s) = 0$, \autoref{eq:kkt}.

Note that the upper bound for the strong rule condition, the right-hand side of \autoref{eq:strong}, is smaller than the upper bound for the KKT condition inequality, i.e., $(1-\alpha)(2\lambda_s - \lambda_{s-1}) < (1-\alpha)(\lambda_{s-1})$, since $\lambda_{s} < \lambda_{s-1}$. We see from this that the strong rule condition, which is a condition on the subdifferential at $\lambda_{s-1}$, is stronger than the zero KKT condition at $\lambda_{s-1}$; the logic of the strong rule is that, if group $k$ satisfies the zero KKT condition at $\lambda_{s-1}$, with some wiggle room in the inequality, and if the function $c_k(\lambda)$ which characterizes the zero KKT condition does not change too much going from one lambda to the next, then the zero KKT will be satisfied for the next value $\lambda_s$.

Finally, we should reiterate that it is possible for the strong rule to make a mistake, the Lipschitz assumption is just that, an assumption. Because of this, it is critical that, after discarding some of the groups and running the algorithm on the others, the zero KKT condition is checked on all those discarded groups to make sure that the strong rule made the right decision, and that those groups are actually zero. Violators of the KKT condition should be put in 'active' status and the algorithm re-run with those groups included. The details of this are in the next section.

Now that we understand the KKT check and strong rule check on groups, we briefly discuss some notation which is used in the next section. In this section we defined a function $c_k(\lambda)$ that was parameterized by the group $k$ and taking in as an argument $\lambda$. We will change emphasis and define a function:
\[
\mathcal{S}_{\lambda}(A),
\]
that takes in a subset of the set of groups, $A$, and returns the set of all elements of $A$ which \textbf{fail} the strong rule check at $\lambda$. This is precisely the set of groups that we suspect of being active, that we should run the update step on. The reason why we want this notation is that it is a waste of time to run the strong rule check on those already known or suspected to be active. 

Similarly, we will define the function 
\[
KKT_{\lambda}(A)
\]
that takes in a  set of groups, and returns the subset of those elements of $A$ that \textbf{fail} the 0-KKT check, i.e. those elements $k \in A$ for which $\norm{c_k(\lambda)}_2 > (1-\alpha)\lambda$. Again, we do not need to check the KKT conditions for those groups on which the update step was run until convergence, so we will want to specify the subset of groups that we need to run the KKT check on after convergence of $U$.


## The four-step algorithm

The algorithm called four-step algorithms in our package performs the sparse group lasso. This algorithm uses the majorized coordinate-descent update step $U$ from section 2.1, and the order of when it stores the active and potentially active groups guarantees the robustness when speeding up the computation. Before discussing it in detail, we need some notations and explanations.


First, recall from the end of the previous section that $\mathcal{S}_{\lambda}(A)$ is a function which performs the strong rule check on a set of groups $A$ at $\lambda$, and returns the set $A^{\prime}$ of groups in $A$ that fail the strong rule check. For each individual group in the set $A$, it checks the inequality in \autoref{eq:strong}. Recall that the strong rule check at a given $\lambda_k$ actually involves both $\lambda_{k}$ and $\lambda_{k-1}$; this is suppressed in this notation, with the consensus.

Similarly, the function $\textrm{KKT}_{\lambda} (A)$ performs the KKT check described in the previous section, and returns the subset $A^{\prime}$ of groups that fail the KKT check. The idea for both of these functions is to keep track of which groups are potentially active (nonzero) for the given $\lambda$. 

Both of these functions take in only a subset of the groups of coefficients, rather than being run on the whole set each time; this is to save time and reduce redundancy. It would not make sense to run a KKT check on those groups that are already known to be active (for the given $\lambda$), because the coordinate descent algorithm is guaranteed to converge for the groups on which is is run. Similarly for the strong rule check, it would be a waste of time to run that inequality on groups that are already known or suspected of being active, since the point of the strong rule check is to separate those suspected of being active from those that are not, and only updating the coefficient estimates for the suspected active groups.

We will keep track of active/potentially active groups with a set $\mathcal{E}$; this is the set of all groups that have failed the strong rule check, or have failed the KKT check, and it is precisely this set of groups that we perform the update step on until convergence. This set is ever-increasing, that is, if a group fails the strong rule or KKT check at some $\lambda_s$ and is put in $\mathcal{E}$, it stays in $\mathcal{E}$ for every subsequent $\lambda_{s+i}$. It is possible for a group to be active for some value of lambda and then become inactive later on in the lambda path, but this is unlikely, and we are not losing too much efficiency by making this group ever-increasing.

Finally, in the convergence step of the algorithms, we want to keep track of how much $\vbeta$ is moving; if it appears to have found a minimum, and is not moving much from the update step, we want to declare convergence and kick out of the updating step. Because of this, we keep track of the maximum change in the $\vbeta^{(k)}$'s with $\max_{k\in\mathcal{E}} \left(\norm{\hat{\vbeta}^{(k)}_{\{\lambda_i\}} - U\left(\hat{\vbeta}^{(k)}_{\{\lambda_i\}}\right)}\right)$. Once $\max_{k\in\mathcal{E}} \left(\norm{\hat{\vbeta}^{(k)}_{\{\lambda_i\}} - U\left(\hat{\vbeta}^{(k)}_{\{\lambda_i\}}\right)}\right) < \epsilon$, where $\epsilon$ is some pre-determined small amount, we move to the next step.


The highlight of four-step algorithm, \autoref{alg:fourStep} is that instead of adding groups into the 'active' set $\mathcal{E}$ before running the algorithm, the algorithm is first run until convergence on the original set $\mathcal{E}$, and the strong rule is run afterwards. The details are as follows.

\begin{algorithm}[tb!]
  \caption{The four-step Algorithm (revised)}
  \label{alg:fourStep}
  \begin{algorithmic}[1]
    \STATE Initialize $\boldsymbol{\lambda} = \{\lambda_s\}_{s = 1}^n$, and the corresponding coefficients at each $\lambda$ value $\{\hat{\vbeta}_{\{\lambda_s\}}\}_{s = 1}^n$, where $n\in\mathbb{N}$.
    \STATE Let $\epsilon$ denote the convergence tolerance, $\mathcal{E}$ be the active set starting from an empty set, and $k$ indicate the groupings.
    \FOR{$s = 1, 2, \cdots, n$}
    \STATE Run through $\mathcal{E}$ (if not empty) until convergence:
    \bindent
    \WHILE{$\max_{k\in\mathcal{E}} \left(\norm{\hat{\vbeta}^{(k)}_{\{\lambda_s\}} - U\left(\hat{\vbeta}^{(k)}_{\{\lambda_s\}}\right)}\right) >\epsilon$}
    \bindent
    \STATE Update $\hat{\vbeta}^{(k)}_{\{\lambda_s\}}$ by $U\left(\hat{\vbeta}^{(k)}_{\{\lambda_s\}}\right)$ for all $\hat{\vbeta}^{(k)}_{\{\lambda_s\}}\in\mathcal{E}$
    \eindent
    \ENDWHILE 
    \eindent
    \STATE Run strong rule check on inactive groups, then the KKT check on the violators:
    \bindent
    \IF {$KKT_{\lambda_s}(S_{\lambda_s}(\mathcal{E}^c))$}
    \bindent
    \STATE $\mathcal{E} \leftarrow \mathcal{E} \cup KKT_{\lambda_s}(S_{\lambda_s}(\mathcal{E}^c))$
    \STATE Return to Line 4.
    \eindent
    \ENDIF
    \eindent
    \STATE Run KKT rule check on whole inactive set:
    \bindent
    \IF {$KKT_{\lambda_s}(\mathcal{E}^c)$}
    \bindent
    \STATE $\mathcal{E} \leftarrow \mathcal{E}\cup KKT_{\lambda_s}(\mathcal{E}^c)$
    \STATE Return to Line 4.
    \eindent
    \ENDIF
    \eindent
    \ENDFOR
    \RETURN{$\hat{\vbeta} \leftarrow \{\hat{\vbeta}_{\{\lambda_s\}}\}_{s = 1}^n$}
  \end{algorithmic}
\end{algorithm}


Assume we have solved the problem for $\lambda_{s-1}$. We loop over $\mathcal{E}$, which was passed over from $\lambda_{s-1}$, and update until convergence. The difference here is that we do not perform the strong check first. After we have run the update loop until convergence on the active set, we then perform the strong rule on the complement, $\mathcal{E}^c$. Since the strong rule depends on the computation of $\vbeta_{\{\lambda_{s-1}\}}$, it doesn't actually matter whether the coefficients are converged or not. For any groups that fail the strong rule check, we run the KKT check; if there are any violators, put them in $\mathcal{E}$ and go back to step 1. Therefore, the difference is that the condition for adding groups to $\mathcal{E}$ is more stringent and is done after obtaining the converged coefficients. Otherwise, we run the KKT check on the full complement $\mathcal{E}^c$. Again, if there are any violators, we put them in $\mathcal{E}$ and go back to check if coefficients are diverged. If there are no violators, we are done, and we move on to the next $\lambda$ in the sequence.

In this algorithm, we are careful to put potentially active groups into the active set. Instead of putting groups in $\mathcal{E}$ immediately after applying the strong rule, we are hesitant to commit to running the algorithm on the strong rule alone. We run the strong rule after convergence, but then double-check that the suspected 'active' groups are actually so by using the KKT check.


# Example usage

This section provides a simple illustration for how to use the \pkg{sparsegl} package \citep{R-sparsegl} for fitting the regularization path for sparse group-lasso penalized learning problems. We first examine the linear regression model when response variable is continuous and then briefly go over the logistic regression case. The package has been submitted to the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=sparsegl and can be installed and loaded in the usual manner:[^devel]

```{r install, eval=FALSE, echo=TRUE}
install.packages("sparsegl")
library(sparsegl)
```

[^devel]: The development version of the package is hosted on Github: https://github.com/dajmcdon/sparsegl.



We first create a small simulated dataset along with a vector indicating the grouping structure.

```{r, data-simulation}
set.seed(1010)
n <- 100
p <- 200
X <- matrix(data = rnorm(n*p), nrow = n, ncol = p)
beta <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), 
          c(2, -3, 8, 0, 0), rep(0, (p - 20)))
groups <- rep(1:(p / 5), each = 5)
eps <- rnorm(n, mean = 0, sd = 1)
y <- X %*% beta + eps
pr <- 1 / (1 + exp(-X %*% beta))
y0 <- rbinom(n, 1, pr)
```

The \pkg{sparsegl} package is mainly used with calls to 2 functions:  

* `sparsegl()`: fits sparse group regularized regression and classification models;
* `cv.sparsegl()`: repeatedly calls `sparsegl()` for the purposes of tuning parameter selection via cross validation.

The interface is intended to closely mimic that available in other \proglang{R} packages for regularized linear models, most notably \pkg{glmnet} \citep{R-glmnet}.
To perform the regularization path fitting at a sequence of regularization parameters, `sparsegl()` takes as required inputs, only `x`, the design matrix, and `y`, the response vector. Other optional arguments are the grouping vector `group`, the `family` (either `"gaussian"` or `"binomial"`), a penalty vector for group weights other than the size, the relative weight of lasso penalty, desired lower or upper bounds for coefficient estimates, and other optional configurations.

<!--
\textbf{sparsegl} object and its associated attributes. One of the main attribute of this object, \textbf{df}, is a vector representing the number of nonzero coefficients at each $\lambda$ value and also an approximation to the exact degree-of-freedom.
-->


```{r, eval=TRUE, echo=FALSE}
library(sparsegl)
```

```{r}
fit <- sparsegl(X, y, group = groups, family = "gaussian")
```

\pkg{S3} methods for the `sparsegl` object are `plot()`, `coef()`, `predict()` and `print()`. The `plot()` function displays either the coefficients or the group norms on the $y$-axis against either $\{\lambda_s\}_{s=1}^S$ or the scaled penalty on the $x$-axis.

```{r coef-trace, fig.width = 4, fig.height = 2}
plot(fit, y_axis = "coef", x_axis = "penalty", add_legend = FALSE)
plot(fit, y_axis = "group", x_axis = "lambda", add_legend = FALSE)
```

The `coef()` and `predict()` methods compute, respectively, the coefficients and predictions $\widehat{\vY}$ given a matrix $\tilde{\mX}$ at the requested $\lambda$s, potentially allowing for $\lambda$ values different from those used at the fitting stage. 

```{r, message=FALSE}
coef(fit, s = c(0.02, 0.03))[c(1,3,25,29),] # display a few
predict(fit, newx = tail(X), s = fit$lambda[2:3]) 
print(fit)
```

The `cv.sparsegl()` function implements $K$-fold cross validation and has a similar signature to `sparsegl()`. It allows the user to choose the number of splits and the loss function for measuring prediction or classification accuracy on the held-out sets.
Here, the S3 `plot()` method displays the cross-validation curve with upper and lower confidence bounds plots calculated as $\pm 1$ standard error across the folds for each $\lambda$ in the regularization path. 

```{r cv-plot, fig.width = 4, fig.height = 2}
cv_fit <- cv.sparsegl(X, y, groups)
plot(cv_fit)
```

The `coef()` and `predict()` methods work similarly to those above. The only differences are that they can accept strings `lambda.min`, the $\lambda$ minimizes the average cross validation error, or `lambda.1se`, the largest $\lambda$ such that the error is within 1 standard error or the minimum.

```{r}
coef(cv_fit, s = "lambda.1se")[c(1,3,25,29),]
predict(cv_fit, newx = tail(X), s = "lambda.min")
```

For logistic regression, only a different `family` is required. Cross validation
can be implemented with misclassification or deviance loss.

```{r logitres, fig.width = 4, fig.height = 2}
fit_logit <- sparsegl(X, y0, groups, family = "binomial")
cv_fit_logit <- cv.sparsegl(X, y0, groups, family = "binomial", 
                            pred.loss = "misclass")
plot(cv_fit_logit, log_axis = "none")
```

In some cases, when computations are at a premium, cross validation my be too demanding for the purposes of risk estimation. For this reason, \pkg{sparsegl} provides a `estimate_risk()` function. It can be used to compute AIC \citep{Akaike1973}, BIC \citep{Schwarz1978}, and GCV \citep{golub1979generalized}information criteria. All three are computed by combining the
log-likelihood with a penalty term for model flexibility. In addition to a fitted `sparsegl` model, `estimate_risk()` also needs the original data. The three available penalties are

- `"AIC"` (Akaike information criterion): $2\textrm{df} / n$;
- `"BIC"` (Bayesian information criterion): $\log(n)\textrm{df} / n$;
- `"GCV"` (Generalized cross validation): $-2\log(1 - \textrm{df} / n)$;  

where df is the degree-of-freedom, and $n$ is the sample size. Because the exact degrees-of-freedom can be computationally expensive, setting `approx_df=TRUE` uses the number of non-zero coefficient estimates, which is a fairly accurate approximation.

```{r risk-estimate, fig.width = 4, fig.height = 2}
er <- estimate_risk(fit, X, y, c("AIC", "BIC"))
```

```{r re-plot, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 2}
library(tidyverse)
er <- er %>% select(-df) %>% pivot_longer(-lambda, values_to = "risk")
err <- er %>% group_by(name) %>% summarise(lambda = lambda[which.min(risk)])
ggplot(er, aes(lambda, risk, color = name)) +
  geom_line() +
  scale_color_manual(values = c("darkblue", "orange")) +
  geom_vline(data = err, aes(xintercept = lambda, color=name), linetype="dashed") +
  theme_bw() +
  ylab("Estimated risk") +
  xlab("Lambda") +
  theme(legend.position = "none") +
  annotate("label", x=c(.32, .03), y=c(5,2.5),color=c("darkblue", "orange"),
           label = c("A I C", "B I C")) +
  scale_x_log10()
```

In this simulation, the $\lambda$ that minimizes AIC is `r round(err$lambda[1],3)` while the CV minimizer is `r round(cv_fit$lambda.min,3)`.

Additional documentation and examples are provided on the package [website](https://dajmcdon.github.io/sparsegl).

# Application to tractography

<!--

# Comparisons with other packages

As we have stated in the beginning, \pkg{SGL} package is computationally inefficient when dealing with a high-dimensional optimization problem, while \pkg{gglasso} does not produce within-group sparsity during the modeling process. To verify that our package is time-saving and in the meanwhile, producing both within-group sparsity and between-group sparsity, we will repetitively generate regression datasets such that they include different sample sizes or a different number of features with or without within-group sparsity. In particular, we will evaluate and compare the time efficiency by \pkg{sparsegl}, \pkg{SGL} and \pkg{gglasso} for between-group sparsity cases, while \pkg{sparsegl}, \pkg{SGL} for including both between-group and within-group sparsity cases when the response variable is either continuous or binary. The elapsed time (in milliseconds) will be measured using \pkg{microbenchmark}, and the results will be visualized with boxplots.

The approach to simulating models is as follows: for each dataset, we first generate an input matrix $\mX$ by randomly sampling numbers from the standard normal distribution and converting them into a $n$-by-$p$ matrix, and the response vector depends on the regression model used in the optimization problem. For linear regression models, the vector $\vY$ is generated linearly by $\vY = \mX\vbeta^* + \boldsymbol\epsilon$, where the error term $\epsilon$ is also generated from the standard normal distribution as white noise. For logistic regression models, $vY$ is a vector containing binary values only. A vector of probabilities with length $n$, namely $\frac{1}{1 + \exp(-\mX\vbeta^*)}$ should be calculated, and each $y_i$ is randomly generated by Bernoulli distribution with the corresponding probability. The entries of coefficients vector $\vbeta^*$ from both models are zeros or nonzero, which is generated depending on whether within-group sparsity is considered. For the coefficients in the group $k$, all the entries of $\vbeta^{*(k)}$ are zeros or nonzeros if the grouping structure is not within-group sparse, otherwise, $\vbeta^{*(k)}$ could a subvector with a mixture of zero and nonzero values. We consider the following combinations of (n, p, within-group sparsity):

\begin{itemize}
\item $n = 100$, $p = 20, 60, 100, 200, 300, 500$ and within-group sparsity exists.
\item $p = 100$, $n = 20, 60, 100, 200, 300, 500$ and within-group sparsity exists.
\item $n = 100$, $p = 20, 60, 100, 200, 300, 500$ and within-group sparsity not exists.
\item $p = 100$, $n = 20, 60, 100, 200, 300, 500$ and within-group sparsity not exists.
\end{itemize}

For each combination, the elapsed time (in milliseconds) is recorded for fitting regularization path at 100 $\lambda$s' values by constructing models using \pkg{sparsegl}, \pkg{SGL} and \pkg{gglasso} in terms of the existence of within-group sparsity over 50 independent datasets. The boxplots illustrate the consumed time on each run.


Figure 1 incorporates all the cases above and plots the change in running time (in log scale) against different sample sizes $n$ or the number of features $p$. When implementing linear regression models, we notice that three packages have similar performance and consume approximately similar time when varying $n$ and $p$, but \pkg{SGL} always requires longer running time when moving to larger $n$ and $p$. In particular, the trajectories of \pkg{gglasso} and \pkg{sparsegl} are similar as shown in plots when not taking within-group sparsity into account. However, a remarkable improvement is demonstrated when building logistic regression models using \pkg{sparsegl}, which takes less time compared to both \pkg{gglasso} and \pkg{SGL}. The increment of time consumption is also comparably small when increasing $n$ and $p$ compared with the other two packages in this example. Overall, \pkg{sparsegl} makes a significant contribution as compared to the existing packages that address sparse group-lasso high-dimensional optimization problems, especially in the logistic regression.

```{r,echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, fig.cap = "Running Time Comparison", fig.width = 5, fig.height = 9}
 source("helper_function.R")
 df <- readRDS("./running-time-records-current.rds")
 plot_box(df)
```
-->

# Discussion and future work

We developed this package for solving sparse group lasso optimization problem based on \textbf{four-step algorithm} with a convex loss function and a combination of $l1$ and $l2$ penalties. This preserves the grouping structure for coefficients by producing both among- and within- group sparsity, and also makes a huge improvement on computational efficiency compared to other existing sparse group lasso packages, which is achieved by applying strong rule check and KKT check iteratively.


# Acknowledgements {.unnumbered}

Here we list all the packages that we needed to compile this code. \pkg{devtools} [@R-devtools] \pkg{usethis} [@R-usethis] \pkg{testthat} [@R-testthat] \pkg{knitr} [@R-knitr] \pkg{microbenchmark} [@R-microbenchmark] \pkg{ggplot2}[-@R-ggplot2] and \pkg{rticles} [-@R-rticles]. Also NSF and NSERC funding.


