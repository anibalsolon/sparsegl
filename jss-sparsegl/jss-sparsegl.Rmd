---
documentclass: jss
author:
  - name: Daniel McDonald
    affiliation: |
      | University of British Columbia
    address: |
      | Department of Statistics
      | Vancouver, BC Canada
    email: \email{daniel@stat.ubc.ca}
    url: \url{https://dajmcdon.github.io/}
  - name: Xiaoxuan Liang
    affiliation: 'University of British Columbia \AND'
    address: |
      | Department of Statistics
      | Vancouver, BC Canada
  - name: Aaron Cohen
    affiliation: 'Indiana University'
    address: |
      | Department of Statistics
      | Bloomington, IN USA
title:
  formatted: "Estimating Sparse Group Lasso with the \\pkg{sparsegl} \\proglang{R} Package"
  # If you use tex in the formatted title, also supply version without
  plain:     "Sparse Group Lasso with \\pkg{sparsegl}"
  # For running headers, if needed
  short:     "\\pkg{sparsegl}: Sparse Group Lasso"
abstract: >
  The sparse group lasso is a high-dimensional regression technique that is
  useful for problems whose covariates have a naturally grouped structure, and
  where sparsity is encouraged at both the group and individual covariate
  level. In this paper we discuss a new \proglang{R} package for computing
  such regularized models. The intention is to provide highly optimized 
  solution routines enabling analysis of very large datasets, especially
  in the context of sparse design matrices.
keywords:
  # at least one keyword must be supplied
  formatted: [generalized linear model, reqularized, strong rule, sparse matrix]
  plain:     [generalized linear model, reqularized, strong rule, sparse matrix]
preamble: >
  \usepackage{amsmath}
  \input{jss-sparsegl-preamble}
output: rticles::jss_article
bibliography: [sparsegl.bib, pkgs.bib]
keep_tex: true
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, fig.path = "fig/")
options(prompt = 'R> ', continue = '+ ')
ggplot2::theme_set(ggplot2::theme_bw(base_family = "Palatino"))
# Add any packages we want to cite to the list below. Don't edit the bib
# manually, or changes will be overwritten. Add other references to
# `sparsegl.bib`
# 
# Note: tags are R-<pkg>
knitr::write_bib(c("devtools", "knitr", "testthat", "usethis",
                   "rticles", "sparsegl", "glmnet",
                   "SGL","gglasso", "msgl", "biglasso",
                   "microbenchmark", "RSpectra",
                   "ggplot2", "dotCall64"),
                 file = "pkgs.bib")
```

# Introduction

\nocite{R-gglasso,R-SGL}

Regularized linear models are now ubiquitous tools for prediction and increasingly inference. When solving such high-dimensional learning problems, adding regularization helps to reduce the chances of overfitting and improve the model performance on unseen data. Sparsity inducing  $\ell_1$-type penalties such as the lasso \citep{tibshirani1996regression} or the Dantzig selector 
\citep{CandesTao2007} perform both variable selection and shrinkage, resulting in near-optimal
statistical properties. The group lasso \citep{yuan2006model} modifies the regularizer, replacing the $\ell_1$ penalty with a groupwise sum of $\ell_2$ norms. When covariates have natural groupings, such as with genomics data or one-hot encoded factors, this group penalty is preferable, because the resulting estimate will include or exclude entire groups of covariates. To simultaneously attain sparsity at both group and individual feature levels, \citet{simon2013sparse} proposed the sparse group-lasso, a convex combination of the $\ell_1$ lasso penalty and the group lasso-penalty. 

While a number of packages exist for solving the sparse group lasso, our \proglang{R} implementation in \pkg{sparsegl} is designed to be fast, especially in the case of large, sparse covariate matrices. This package focuses on finding the optimal solutions to sparse group-lasso penalized learning problems at a sequence of regularization parameters, implements risk estimators in an effort to avoid cross validation if necessary, leverages a fast, compiled \proglang{Fortran} implementation, avoids extraneous data copies, and undertakes a number of additional computational efficiency improvements.
In \proglang{R}, there are already excellent implementations of sparse group lasso and group lasso, namely \pkg{SGL}, \pkg{gglasso}, and \pkg{biglasso}. Of these, only \pkg{SGL} employs the additional $\ell_1$ sparsity-inducing penalty. However, it has a number of drawbacks that result in much slower performance, even on small data. One major reason is the omission of so-called "strong rules" \citep{tibshirani2012strong} that help coordinate descent algorithms to avoid many of the groups which will turn out to have zero coefficient estimates. The \pkg{gglasso} \citep{R-gglasso,yang2015fast} and \pkg{biglasso} \citep{R-biglasso,zeng2020biglasso} packages are both computationally fast. The former incorporates the strong rule, and the latter involves a hybrid safe-strong rule along with scalable storage and a parallel implementation in \proglang{C++} and \proglang{R} that allows for data that exceeds the size of installed random access memory. Unfortunately, neither allows within-group sparsity (i.e., they performs group lasso, not sparse group lasso). Thus, the estimated coefficients produced by these packages will have some active groups and some inactive groups, but within an active group, generally all the coefficients will be nonzero. 



<!-- \pkg{msgl} \citep{vincent2014sparse} is also an \proglang{R} package solving sparsel group lasso multiclassification problems via fitting multinomial logistic regression model. This package overcomes the nondifferential penalty problem at zero and since the algorithm is Newton-based, it substitutes the Hessian matrix with an upper bound on that, which reduces the costs of computation. However, the multinomial logistic regression is an extension to the binary logistic regression, allowing for taking the response variable with multiple classes, which consumes an excessive amount of time for model fitting when processing binary response variable. -->


In \proglang{python}, \pkg{asgl} \citep{civieta2020adaptive} implements adaptive sparse group-lasso, which flexibly adjusts the weights in the penalization on the groups of features. Additionally it incorporates quantile loss. As with the other packages mentioned above, it can also solve the special cases (lasso and group lasso). However, for all optimization problems, it directly uses \pkg{cvxpy} without strong rules or other tricks to relate solutions to each other across values of the tuning parameter.

<!-- The paper \citep{ida2019fast} also brings up a fast algorithm applying KKT and revised KKT checks repetitively through block coordinate descent for computing sparse group-lasso model. In this algorithm, the main feature is that the revised KKT check first searches through all groups to filter out the groups whose feature coefficients are zero, then the original KKT check confirms the actual nonzero coefficient groups among the remaining candidate active groups. The revised KKT check could remarkably reduce the time complexity, which is introduced by approximating a metric used to compare against a fixed threshold with a tight upper bound.-->

Our contribution, then, is to provide a package that performs sparse group lasso, and is faster than existing implementations. In particular, \pkg{sparsegl} has the following benefits:
\begin{itemize}
\item Performs Gaussian and logistic regression;
\item Allows for interval constraints on the coefficients;
\item Accommodates a sparse design matrix and returns the coefficient estimates in a sparse matrix;
\item Uses strong rules for fast computation along a sequence of tuning parameters;
\item Uses \pkg{dotCall64} to interface with low-level \proglang{Fortran} functions and avoid unnecessary copying as well as allow for 64-bit integers \citep[see][]{gerber2017dotcall,gerber2018dotcall};
\item Provides information criteria as risk estimators (AIC/BIC/GCV);
\end{itemize}
A comparison of features of this and related \proglang{R} packages is shown in Table \ref{tab:comparison}. In Section \ref{methodology-estimation-and-prediction}, we describe the algorithmic implementation in detail, paying particular attention to the strong rule. In Section \ref{example-usage}, we show how to use the package, running through an example with simulated data. Section \ref{applications} demonstrates many of the
unique features of \pkg{sparsegl} in two applications.
<!-- In Section \ref{comparisons-with-other-packages}, we compare our package to the other existing group-lasso packages, and also compare the two variants of our algorithm with each other. -->
We summarize and discuss paths for future work in Section \ref{discussion-and-future-work}, and make promising proposals for potential future research as well.


\begin{table}
\centering
\label{tab:comparison}
\caption{Summary of features available in this and related \proglang{R} packages.}
\begin{tabular}{lcccccc}
\toprule
& Regression \& & Within group & Sparse & Strong & Avoids & Interval \\
& classification & sparsity & matrices & rules & copies & constraints \\
\midrule
\pkg{sparsegl} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\pkg{gglasso} & \checkmark & & &\checkmark \\
\pkg{SGL} & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}


# Methodology, estimation and prediction

Given a sample of $n$ observations of a univariate response $y_i$ and a corresponding vector of features $\mathbf{x}_i \in \R^p$, the standard linear regression setup has
\begin{equation}
\label{eq:linmod}
y_i = \mathbf{x}_i^\T \vbeta + \sigma\epsilon_i,\ \ i=1,\ldots,n,
\end{equation}
where $\epsilon_i$ is independent standard Gaussian noise and $\sigma > 0$. While ordinary least squares estimates the coefficient vector $\vbeta$ by solving $\min_{\vbeta} \frac{1}{2n}\sum_{i=1}^n (y_i - \vx_i^\T\vbeta)^2$, this method tends to behave poorly if $p \gg n$. In what follows, we will write $\vy = (y_1,\ldots,y_n)$ and let $\mX$ be the rowwise concatenation of $\vx^\T_1,\ldots,\vx_n^\T$.


The lasso addresses this difficulty by adding an $\ell_1$ penalty to the
optimization problem:
\begin{equation}
\label{eq:lasso}
\min_{\vbeta} \left\{\frac{1}{2n} \sum_{i=1}^n (y_i - \vx_i^\T\vbeta)^2 + \lambda\sum_{j=1}^p |\beta_j|\right\} = \min_{\vbeta}\left\{\frac{1}{2n}\norm{\vY-\mX\vbeta}_2^2 + \lambda \norm{\vbeta}_1\right\},
\end{equation}
where $\norm{\cdot}_2$ is the Euclidean ($\ell_2$) norm and $\norm{\cdot}_1$ is the $\ell_1$ norm.
The benefit of this penalty is that it tends to allow only a subset of coefficient estimates to be nonzero, hence performing variable selection. Here, $\lambda$ is a hyperparameter that trades fidelity to the data---small $\lambda$ emphasizes minimization of the squared-error---with desirable regularization that selects a subset of variables and improves prediction accuracy. 

A variant of this, the group lasso \citep{yuan2006model} is appropriate when there is a natural grouping structure for the features. That is, we assume that both the design matrix $\mX$ and the corresponding vector of coefficients can be partitioned into meaningful groups, and, by analogy with lasso regression, only a few of the groups are active, i.e., have nonzero coefficients. The group lasso thus performs regularization that has the effect of discarding groups of predictors rather than the predictors themselves:
\begin{equation}
\min_{\vbeta}\left\{\frac{1}{2n}\big\lVert \vY-\sum_{g=1}^G \mX^{(g)}\vbeta^{(g)}\big\rVert_2^2 + \lambda\sum_{l=1}^m\sqrt{w_g}\snorm{\vbeta^{(g)}}_2\right\}.
\label{eq:group-lasso}
\end{equation}
Grouping may occur naturally---say with the inclusion of many categorical predictors, groups of genes, or brain regions---or may be a design choice using additive models and basis expansions.
Note that the in Equation \ref{eq:group-lasso} grouping structure is explicitly stated: the vector of coefficients, $\vbeta$, is thought of as a concatenation of the coefficient subvectors of the various groups $\vbeta^{(g)}$, and similarly the data matrix $\mX$ is the concatenation of submatrices, each submatrix $\mX^{(g)}$ being composed of the columns that correspond to the particular group. Thus the first part of the equation, $\vY-\sum_{g=1}^G\mX^{(g)}\vbeta^{(g)}$, is identical to the more simply-written equation $\vY-\mX\vbeta$, but the notation serves to emphasise the partitioning.

The penalty however---$\sum_{g=1}^G\sqrt{w_g}\snorm{\vbeta^{(g)}}_2$---is different from the corresponding part in Equation \eqref{eq:lasso}, using instead the sum of the (non-squared) $\ell_2$-norms of the coefficient vectors of the various groups. It is the non-differentiability of this expression at $\mathbf{0}\in\R^{|g|}$ (with $|g|$ meaning the size of group $g$) that accounts for the group-discarding property of the problem, similar to the way that the  non-differentiability the absolute value at $0$ is responsible for discarding individual predictors in the lasso.


As with Equation \eqref{eq:lasso}, there is only a single tuning parameter $\lambda$, whose value determines the strength of regularization. Within the second summation are the relative weights of the groups, $w_g$. These are often taken to be the size of the corresponding group. For simplicity, this notation is suppressed below where the meaning is clear.

Finally, in a group-structured problem as above, it may be desirable to enforce sparsity, not only among the groups, but also within the groups. The sparse group lasso \citep{simon2013sparse} does this by combining the penalties in Equations \eqref{eq:lasso} and \eqref{eq:group-lasso}:
\begin{equation}
  \label{eq:sparsegl}
\min_{\vbeta}\left\{\frac{1}{2n}\snorm{\vY-\mX\vbeta}_2^2 + (1-\alpha)\lambda\sum_{g=1}^G \snorm{\vbeta^{(g)}}_2+\alpha\lambda\snorm{\vbeta}_1\right\}.
\end{equation}
There is now a second tuning parameter $\alpha$, which controls the relative emphasis of intra- vs inter-group sparsity in the predictors.

<!--
The solution to the sparse group lasso in Equation \eqref{eq:sparsegl}, like the other two procedures, does not have a closed form expression and thus relies on algorithmic implementations, which may be computationally expensive for large datasets and/or many values of $\lambda$. The rest of this paper therefore concerns the mitigation of this issue, in particular the following question: if we are solving the sparse group lasso, not for a single value of the tuning parameter $\lambda$ but a whole parameter space---in this context, a range of values $\{\lambda_1,\dots \lambda_S\}$--- can we shorten the total computational time by making use of the already-computed solution at previous $\lambda$s' in the parameter space to speed up computation at a given $\lambda$?

In this paper we make use of a heuristic called the Strong Rule \citep{tibshirani2012strong} to help solve this problem, with some success. The strong rule uses the solution at the previous $\lambda_{s-1}$ to predict which groups will remain inactive upon solution at the current $\lambda_s$, and discards those groups before entering into the algorithm. If a significant number of groups are thrown out before entering the algorithm, convergence time can improve significantly. The \texttt{R} package we have created has an algorithm called \textbf{four-step algorithm}, which uses the strong rule. 
-->


## The group-wise majorization-minimization algorithm

There is no closed-form solution to the optimization problem in Equation \eqref{eq:sparsegl}, so we require a numerical procedure. Because the problem is convex, a variety of methods may be used.
The general framework for our algorithm is the same as the majorized block-wise coordinate descent algorithm developed in \citep{yang2015fast, simon2013sparse}. What this means is that, for a fixed value of $\lambda$, we loop over the groups and update only those coefficients while holding all other groups constant. In particular, instead of using the exact Hessian to determine the step size and direction in every update step, we update according to a simpler expression that majorizes the objective. 

For the rest of this section, we describe this majorization algorithm, focusing on a particular group $k$ and holding the coefficients for all other groups fixed. We note here that, because the loss function in Equation \eqref{eq:sparsegl} is differentiable and the penalty terms are convex and separable (i.e., they can be decomposed into a sum of functions each only involving a single group), this block coordinate descent algorithm is guaranteed to converge to a global optimum \citep{tseng2001convergence}.

To begin with, we introduce some notation. Let 
\[
\vR_{(-k)} = \vY - \sum_{g \neq k} \mX^{(g)} \vbeta^{(g)}
\]
be the partial residual without group $k$ where all the group fits besides that of group $k$ are subtracted from $\vY$. With all other groups held fixed, we aim to solve:
\begin{equation}
	\label{eq:sparsegroupk}
\min_{\vbeta^{(k)}} \frac{1}{2n} \snorm{\vR_{(-k)}-\mX^{(k)}\vbeta^{(k)}}_2^2 + (1-\alpha)\lambda \snorm{\vbeta^{(k)}}_2 + \alpha \lambda \snorm{\vbeta^{(k)}}_1. 
\end{equation}
In what follows, we will suppress the $(k)$ notation, with the understanding that we are really referring to only the $k^\textrm{th}$ group of the coefficient vector and the partial residual $\vR_{(-k)}$. We will also define, the unpenalized loss function
\[
\ell (\vbeta) = \frac{1}{2n}\snorm{\vR - \mX\vbeta}_2^2, 
\]
so that our objective function for the $k^\textrm{th}$ group becomes $\ell (\vbeta) + (1-\alpha)\lambda\norm{\vbeta}_2 + \alpha \lambda \norm{\vbeta}_1$, and we are interested in finding an optimal value, $\hat{\vbeta}$. This enables the procedure to generalize easily to logistic loss or, in principle, other exponential families.

Any global minimum must satisfy a subgradient equation, similar to a first-derivative test for an optimum, except that $\norm{\cdot}_2$ and $\norm{\cdot}_1$ are non-differentiable at $\mathbf{0}$. For Equation \eqref{eq:sparsegroupk} above, taking the subdifferential and setting equal to zero gives us the following first-order condition: 
\begin{equation}
  \label{eq:subgrad}
\nabla \ell(\vbeta) = (1-\alpha)\lambda \mathbf{u} + \alpha \lambda \mathbf{v},
\end{equation}
where $\mathbf{u}$ is the subgradient of $\norm{\vbeta}_2$ and $\mathbf{v}$ is the subgradient of $\norm{\vbeta}_1$. The first is defined to be $\vbeta
/ \norm{\vbeta}_2$ if $\vbeta$ is a nonzero vector, and is any vector in the set $\{\mathbf{u} : \norm{\mathbf{u}}_2 \leq 1 \}$ otherwise; the second, $\mathbf{v}$, is defined coordinate-wise as $v_j = \text{sign}(\beta_j)$ if $v_j \neq 0$, and is any value $v_j \in \{v_j : |v_j| \leq 1 \}$ otherwise.


<!--At this point, it is possible to use \autoref{eq:subgrad} to derive an update step and develop a cyclical coordinate-wise algorithm, but it is too computationally expensive, involving repeated large-matrix multiplication (\citep{simon2013sparse}); instead we use the majorization-minimization idea discussed above.-->

For the Gaussian case, the unpenalized loss $\ell (\vbeta)$ is a quadratic function in $\vbeta$, so it is equal to its second order Taylor expansion about any point $\vbeta_0$ in the parameter space. We thus start with the following equality for any given $\vbeta_0$ (recalling that $\vbeta_0$ here is really only for group $k$):  
\begin{equation}
\forall \vbeta,\ \vbeta_0,\quad \ell(\vbeta) = \ell(\vbeta_0)+(\vbeta - \vbeta_0)^\T\nabla \ell(\vbeta_0)+\frac{1}{2}(\vbeta - \vbeta_0)^\T \Hessian (\vbeta - \vbeta_0),
\label{eq:TaylorExp}
\end{equation}
where the gradient $\nabla \ell$ is the first total derivative of $\ell$ (evaluated at $\vbeta_0$) and $\Hessian$, the Hessian, is the second total derivative. For $\ell(\vbeta) = \frac{1}{2n}\snorm{\vR - \mX\vbeta}_2^2$, a short computation shows that the Hessian is $\Hessian = \frac{1}{n}\mX^\T \mX$.

For the large-scale problems motivating this work, the matrix $\mX$ is large, so computing $\mX^\T \mX$, storing it in memory, or inverting it, is computationally prohibitive. Instead, we replace this matrix with a simpler one, $t^{-1}\mathbf{I}$, a diagonal matrix with the value of $t$ selected to be such that this dominates the Hessian (in the sense that $t^{-1}\mathbf{I} - \Hessian$ is positive definite). For our algorithm we choose the largest eigenvalue of the Hessian and use that for $t^{-1}$. Note that this eigenvalue must be computed for each group $g \in G$, but this computation is relatively simple using the power method or other techniques as implemented with \pkg{RSpectra} \citep{R-RSpectra}.
This upper bound leads to the following inequality:
\begin{equation}
\forall \vbeta,\ \vbeta_0,\quad \ell(\vbeta) \leq \ell(\vbeta_0)+(\vbeta - \vbeta_0)^\T\nabla \ell(\vbeta_0)+\frac{1}{2t}(\vbeta - \vbeta_0)^\T  (\vbeta - \vbeta_0).
\label{eq:dominate}
\end{equation}

Replacing the loss function in the original minimization problem in Equation \eqref{eq:sparsegroupk} with the right-hand side of Equation \eqref{eq:dominate}
leads to a majorized version of the original problem
\begin{equation}
\label{eq:Meq}
\ell(\vbeta_0)+(\vbeta - \vbeta_0)^\T\nabla \ell(\vbeta_0)+\frac{1}{2t}\norm{\vbeta_0-\vbeta}_2^2+ (1-\alpha)\lambda\norm{\vbeta}_2+\alpha\lambda\norm{\vbeta}_1,
\end{equation}
which no longer involves operations with the Hessian matrix. 


As before, the optimal value for Equation \eqref{eq:Meq} is determined by its subgradient equation, similar to that of Equation \eqref{eq:subgrad}:
\begin{equation}
\frac{1}{t} (\vbeta - (\vbeta_0 - t\nabla \ell(\vbeta_0))) +(1-\alpha)\lambda \mathbf{u} + \alpha \lambda \mathbf{v} = \mathbf{0},
\end{equation}
with $\mathbf{u}$ and $\mathbf{v}$ as defined above. Solving this for $\vbeta$ in terms of $\vbeta_0$ results in the following expression:
\begin{equation}
\hat{\vbeta} = U(\vbeta_0) =
\left(1-\frac{t(1-\alpha)\lambda}{\norm{S(\vbeta_0-t\nabla \ell(\vbeta_0),\ t\alpha\lambda)}_2}\right)_+ S\big(\vbeta_0-t\nabla \ell(\vbeta_0),\ t\alpha\lambda\big),
\label{eq:updateStep}
\end{equation}
where $(z)_+ = \max\{z,\ 0\}$ and $S$ is the coordinate-wise soft threshold operator, on a vector $\boldsymbol\gamma$ and scalar $b$,
\begin{equation}
\label{softthresh}
(S(\boldsymbol\gamma,b))_j = \text{sign}(\gamma_j)(|\gamma_j| - b)_+,
\end{equation}
i.e., for each coordinate in the vector, it shrinks that coordinate in magnitude by the amount $b$, and sets it to zero if the magnitude of that coordinate was smaller than $b$ to begin with. It is this soft-thresholding operation that encourages within-group sparsity.

An examination Equation \eqref{eq:updateStep} shows that it is possible for the whole group to be set to zero (made inactive) due to the (hard) threshold operator $(\cdot)_+$ in the first part of the expression. It is also possible for individual components of $\vbeta^{(k)}$ to be zeroed out from the coordinate-wise (soft) threshold operator $S$. Therefore, performing this update step tends to enforce coefficient sparsity at both the group- and individual-level.


<!--\autoref{eq:updateStep} defines the update step at a given group $k$, via $\vbeta \leftarrow U(\vbeta)$. At a point $\vbeta_{old} = \hat{\vbeta}^{(k)}_{old}$, we find the convex function $M$ that majorizes the objective function--centered at that $\vbeta_{old}$--and choose the new value of beta to be the minimizer of majorized function, $\vbeta_{new} = U(\vbeta_{old})$. This update step has the advantage that it is easy to compute, and is guaranteed to converge to the optimum $\hat{\vbeta} = \hat{\vbeta}^{(k)}$ for that given group.  -->

\begin{algorithm}[tb!]
  \caption{Sparse group lasso solution for fixed $\lambda$, regression version}
  \label{alg:sparsegl}
  \begin{algorithmic}[1]
    \STATE {\bfseries Input:} $\lambda \geq 0$, $\alpha \in [0,\ 1]$, set of groups $\mathcal{G}$, initial coefficients $\vbeta$, $\mathbf{r} = \vy-\mX\vbeta$
    \WHILE{Not converged}
    \FOR{$g \in \mathcal{G}$}
    \STATE Update $\vbeta^{(g)} = \left(1-\frac{t(1-\alpha)\lambda}{\snorm{S\big(\vbeta^{(g)}-t\nabla \ell(\vbeta^{(g)}),\ t\alpha\lambda\big)}_2}\right)_+ S\big(\vbeta^{(g)}-t\nabla \ell(\vbeta^{(g)}),\ t\alpha\lambda\big)$.
    \STATE Update $\mathbf{r} = \mathbf{r} - \mX\vbeta^{(g)}$.
    \ENDFOR
    \ENDWHILE 
    \RETURN{$\vbeta$}
  \end{algorithmic}
\end{algorithm}


<!-- Both of the algorithms in the \pkg{sparsegl} package use the update step $U$ in \autoref{eq:updateStep}, iterating over all the groups and performing one update per pass. Since the partial residual $\vR_{(-k)}$ is used in $U$ (via $\ell$), we note that, for a given group $k$ at a given point in the cycle, the update $\vbeta^{(k)} \leftarrow U(\vbeta^{(k)})$ uses the new values of $\{\vbeta^{(1)}, \dots \vbeta^{(k-1)}\}$ and the old values for the subsequent $\{\vbeta^{(k+1)},\dots \vbeta^{(m)}\}$. -->

Above, we have focused on the Gaussian linear model with $\ell(\vbeta) = \frac{1}{2n}\snorm{\vR - \mX\vbeta}_2^2$, $\nabla \ell(\vbeta) = -\frac{1}{n}\mX^\T (\vR - \mX\vbeta)$, and $\mathbf{H}\leq t\mathbf{I}$. 
In the case of logistic regression, we use exactly the same procedure but with
$\ell(\vbeta) = \frac{1}{n}\sum_{i}\log(1 + \exp\{-r_i\vx_i^\T\vbeta\})$, $\nabla \ell(\vbeta) = -\frac{1}{n}\sum_{i}y_i\vx_i^\top (1 + \exp\{-r_i\vx_i^\T\vbeta\})^{-1}$, and $\mathbf{H}(\vbeta) \leq 4t\mathbf{I}$. This procedure is explicitly stated in \autoref{alg:sparsegl}.

While the procedure described so far solves Equation \eqref{eq:sparsegl} for fixed choices of $\lambda$ and $\alpha$, the data analyst does not typically know these ahead of time. Rather, we would like to solve the problem for a collection of values of $\lambda$ (and perhaps $\alpha$ as well). It turns out that the structure of this optimization problem allows for some heuristics that can perform this sequential optimization with a minimum of additional computational resources, in some cases, solving Equation \eqref{eq:sparsegl} faster for a sequence of values $\lambda_s \in \{\lambda_1,\ldots,\lambda_S\}$ than for a single choice \citep{tibshirani2012strong}. We describe our implementation of this procedure next.


<!-- Because the optimization problem is convex, with differentiable loss and separable penalty, it can be shown that this type of blockwise coordinate descent algorithm is guaranteed to converge to the optimal solution for the full vector $\hat{\vbeta}$. That is, it does not get `stuck' at any inflection points or local minima. Therefore, if that was all the algorithm did, there would be no need to check the KKT conditions for optimality. -->

<!-- However, our algorithms both make use of the sequential strong rule, a heuristic that predicts, before computation, which groups will remain inactive, and discards them; this results in significant computational savings, but because this pre-processing prediction is not always correct, we will need to do a KKT check \citep{boyd2004convex} after convergence to make sure the initial prediction was accurate. This is described in the next section. -->




## Sequential strong rule and KKT conditions

For any fixed value of $\lambda$, many groups of coefficient estimates will end up being equal to zero. If, somehow, we knew _which_ groups, we could completely avoid visiting them in the blockwise coordinate descent updates, and therefore avoid calculating any portion of Equation \eqref{eq:updateStep}. This would significantly speed up computations. 

Re-examining Equation \eqref{eq:subgrad}, we can see that the first order condition implies that, for each group $g$, any solution must satisfy

\begin{equation}
\label{eq:subgrad-again}
\snorm{S(\nabla \ell(\vbeta_g),\ \lambda\alpha)}_2 \leq (1-\alpha)\lambda.
\end{equation}

This is because, as $\mathbf{u}$ is the subgradient of $\snorm{\vbeta_g}_2$, $\snorm{\mathbf{u}}_2\leq 1$. Furthermore, if $\snorm{\mathbf{u}}_2 < 1$, then $\hat{\vbeta}_j = \mathbf{0}$. In the previous section, we used the sufficiency of the first order Karush-Kuhn-Tucker (KKT) stationarity condition to derive a solution, while Equation \eqref{eq:subgrad-again} is the necessary version. So given a potential solution, it is easy to check its validity. Unfortunately, this is not constructive. 

The sequential strong rule \citep{tibshirani2012strong} begins from Equation \eqref{eq:subgrad-again} and makes use of the fact that we are solving for a sequence $\{\lambda_1 \geq \lambda_2 \geq \dots \geq\lambda_M\}$ of lambda parameters, rather than a single value. At each $\lambda_m$, we rely on the fact that we have solved the problem for the previous $\lambda_{m-1}$, and use this information to quickly discard many groups of predictors. Without loss of generality, for the rest of this section assume that the problem has already been solved for $\lambda_{m-1}$.

Define $c_g(\lambda) = S(\nabla \ell(\vbeta_g),\ \lambda\alpha)$. Now, we make the assumption that $c_g(\lambda)$ is $(1-\alpha)$-Lipschitz, i.e., that
\begin{equation}
\label{eq:lipschitz}
\forall \lambda,\ \lambda^{\prime}\ \  \ \snorm{c_g(\lambda)-c_g(\lambda^{\prime})}_2 \leq (1-\alpha)|\lambda - \lambda'|.
\end{equation}
This is essentially saying that $c_g$ is not only continuous but does not vary too much in $\lambda$.
<!-- this is equivalent to saying that the function is differentiable almost everywhere and has bounded derivative.  -->
This Lipschitz assumption appears unintuitive, and in fact, is not always true, but it turns out to be useful. 

If we knew that $\snorm{c_g(\lambda_m)}_2 < (1-\alpha)\lambda_m$ then we could safely ignore it. But we already have the solution at $\lambda_{m-1}$. By the triangle inequality (first) and the Lipschitz assumption (second),
$$
\snorm{c_g(\lambda_m)}_2 \leq \snorm{c_g(\lambda_m) - c_g(\lambda_{m-1})}_2 +
\snorm{c_g(\lambda_{m-1})}_2 \leq (1-\alpha)(\lambda_{m-1} - \lambda_m) + \snorm{c_g(\lambda_{m-1})}_2.
$$
To continue this upper bound, we want
$(1-\alpha)(\lambda_{m-1} - \lambda_sm) + \snorm{c_g(\lambda_{m-1})}_2 \leq (1-\alpha)\lambda_m$ which holds if
\begin{equation}
\label{eq:strong}
\snorm{c_g(\lambda_{m-1})}_2 \leq (1-\alpha)(2\lambda_m - \lambda_{m-1}).
\end{equation}

Applying this logic to Equation \eqref{eq:Meq} above gives the sequential strong rule for the sparse group lasso:
\begin{equation}
\label{eq:sgl-strong}
\snorm{S\big(\nabla \ell(\vbeta_g),\ t\alpha\lambda_{m-1}\big)}_2 \leq t(1-\alpha)(2\lambda_m - \lambda_{m-1}).
\end{equation}
For more details in related settings, see \citep{tibshirani2012strong}.
If Equation \eqref{eq:strong}
holds, then we ignore group $g$ when solving the problem at $\lambda_m$. That is to say, when we move from $\lambda_{m-1}$ to $\lambda_m$, we first check this condition on all groups, using the previously computed solution for $\hat{\vbeta}(\lambda_{m-1})$, and then perform blockwise coordinate descent, using only those groups that failed this inequality.

This discarding rule is fast, because it uses the previously computed solution and a simple inequality, and in practice, it tends to accurately discard large numbers of groups. However, we should reiterate that it is possible for the strong rule to fail. The $(1-\alpha)$-Lipschitz assumption is not a guarantee. Because of this, it is critical that, after discarding some of the groups and running the algorithm on the others, the KKT condition is checked on all discarded groups. If there are no violations, then we have the solution. 

To minimize gradient computations for groups that will eventually be determined to be inactive, we actually keep track of two sets: the strong set $\mathcal{S}$ and the active set $\mathcal{A}$. The active set collects all groups that have ever had non-zero coefficients at previous values of $\lambda$. We first iterate over previously active groups, then check the strong set to see if we missed any, and finally check all the remaining groups. When $p$ is very large, this avoids onerous computations for as many groups as possible. The complete algorithm is shown in Algorithm \ref{alg:djm}.

\begin{algorithm}[tb!]
  \caption{Sequential strong rule and Majorization Minimization in \pkg{sparsegl}}
  \label{alg:djm}
  \begin{algorithmic}[1]
  \STATE {\bfseries Input:} $\mX$, $\vY$, $\mathcal{G}$, and $\{\lambda_1,\ldots,\lambda_M\}$.\;\; \textbf{Output:} $\hat\vbeta$.
  \STATE {\bfseries Initialize:} $\mathcal{A} = \mathcal{S} = \varnothing$, $\hat\vbeta = 0$.
  \FOR{$m=1$ \TO  $M$} 
  \STATE \textbf{Update} $\mathcal{S} \leftarrow \mathcal{S} \bigcup \left\{
  g\in\mathcal{S}^c : \snorm{S(\nabla \ell(\hat\vbeta_g),\ 
  t\alpha\lambda_m)}_2 > t(1-\alpha)\lambda_m \right\}$.
  \STATE \textbf{Apply} Algorithm \ref{alg:sparsegl} with $\mathcal{G} = \mathcal{A}$ (MM gradient update).
  \STATE \textbf{Update} $\mathcal{A} \leftarrow \mathcal{A} \bigcup \left\{g \in\mathcal{S}\bigcap\mathcal{A}^c : \snorm{S(\nabla \ell(\hat\vbeta_g),\ t\alpha\lambda_m)}_2 > t(1-\alpha)\lambda_m\right\}.$
  \begin{ALC@g}
  \STATE If there were any violations, \textbf{go to} to Line 5.
  \end{ALC@g}
  \STATE \textbf{Update} $\mathcal{A} \leftarrow \mathcal{A}\bigcup \left\{g\in\mathcal{S}^c\cap\mathcal{A}^c : \snorm{S(\nabla \ell(\hat\vbeta_g),\ t\alpha\lambda_m)}_2 > t(1-\alpha)\lambda_m\right\}.$
  \begin{ALC@g}
  \STATE If there were any violations, \textbf{go to} to Line 5.
  \end{ALC@g}
  \STATE \textbf{Set} $\mathcal{S} = \mathcal{S} \bigcup \mathcal{A}$.
  \ENDFOR
  \end{algorithmic}
\end{algorithm}


## Risk estimation

For many regularized prediction methods, tuning parameter selection is largely performed with cross validation. However, cross validation can be computationally expensive when the data set is large enough that the initial fit is slow. For this reason, \pkg{sparsegl} provides information criteria as well as cross validation routines.

In the Gaussian linear regression model given by Equation \eqref{eq:linmod}, if $\sigma$ is
unknown then a general form for a family of information criteria is given by
\begin{equation}
  \label{eq:GICsigUnknown}
  \textrm{info}(C_n,g) 
  =\log\left(\frac{1}{n}\snorm{\vY-\mX\hat\vbeta}_2^2\right) +
  C_n \; g(\df),
\end{equation}
where $C_n$ depends only on $n$, $g: [0,\infty) \rightarrow \mathbb{R}$ is a fixed function, and the degrees of freedom ($\df$) measures the complexity of the estimation procedure.
The choices
$C_n = 2/n$ or $C_n = \log(n)/n$ with $g(x) = x$ are commonly referred to as AIC \citep{Akaike1973} and BIC \citep{Schwarz1978}, respectively.  Additionally, generalized
cross validation \citep[GCV]{golub1979generalized} is defined as
\begin{equation}
\textrm{GCV} = \frac{\frac{1}{n}\snorm{\vY-\mX\hat\vbeta}_2^2}{(1-\df/n)^2}.
\label{eq:gcv}
\end{equation}
Written on the log scale, GCV takes the form of equation \eqref{eq:GICsigUnknown} with $g(x) = \log(1-x/n)$ and $C_n = -2$.

The key components for all three information criteria are the negative log likelihood and the degrees of freedom. The first is simply a function of the in sample (training) error. On the other hand, the degrees of freedom, while simple in the unregularized linear model---it is the number of parameters---is less obvious for the sparse group lasso. In general, the degress of freedom for any predictor $\hat{\vy}$ of $\vY$ is defined as \citep{Efron1986}
$$\mathrm{df}(\hat\vy) = \frac{1}{\sigma^2}\sum_{i=1}^n\mathrm{Cov}(\vy_i,\ \hat\vy_i).$$
In the case of any linear predictor, $\hat\vy = \mathbf{A}\vy$ for some matrix $\mathbf{A}$, it is easy to see that $\mathrm{df} = \mathrm{tr}(\mathbf{A})$.
\citet{vaiter2012degrees} gives an explicit formula for the group lasso (without intragroup sparsity), but only minor modifications are required for the sparse group lasso. We give a simplified version of their result here

\begin{proposition}[\citet{vaiter2012degrees}]
Suppose that for a fixed $\lambda >0$, the support set of $\hat{\beta} = \mathcal{S}$ and that $\mX_{\mathcal{S}}$ is the set of columns associated to $\mathcal{S}$. Assume that $\mX_{\mathcal{S}}$ has full column rank. Then, 
$$\df = \mathrm{tr}\left(\mX_\mathcal{S}\left(\mX_\mathcal{S}^\T \mX_\mathcal{S} + \lambda \mathbf{K}\right)^{-1}\mX_{\mathcal{S}}^\T\right).$$
Here, $\mathbf{K}\in \R^{\mathcal{S}\times\mathcal{S}}$ is a block diagonal matrix with each block corresponding to a group $g$ having at least 1 nonzero $\hat\beta$. For such a group $g$, denote $\hat\beta_{g | \mathcal{S}}$ the subvector of nonzero coefficient estimates. Then $$\mathbf{K}_g = \frac{1}{\snorm{\hat\vbeta_{g | \mathcal{S}}}_2}\left(\mathbf{I} - \frac{\hat\vbeta_{g | \mathcal{S}} \hat\vbeta_{g | \mathcal{S}}^\T}{\snorm{\hat\vbeta_{g | \mathcal{S}}}^2_2} \right).$$
\end{proposition}

As long as the number of nonzero coefficients $|\mathcal{S}|$ is reasonably small, the degrees of freedom can be efficiently calculated for each value of $\lambda$. However, this calculation is generally cubic in $|\mathcal{S}|$. In these cases, an approximation may be desired. We have found, in practice, that $\lambda \mathbf{K}\approx \mathbf{0}$ is reasonably accurate, suggesting that $\mathrm{df} \approx |\mathcal{S}|$ is also reasonable. This approximation is exact for the lasso with $\alpha=1$ \citep{ZouHastie2007}.



# Example usage

This section provides a simple illustration for how to use the \pkg{sparsegl} package \citep{R-sparsegl} for fitting the regularization path for sparse group-lasso penalized learning problems. We first examine the linear regression model when response variable is continuous and then briefly go over the logistic regression case. The package has been submitted to the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=sparsegl and can be installed and loaded in the usual manner:[^devel]

```{r install, eval=FALSE, echo=TRUE}
install.packages("sparsegl")
library(sparsegl)
```

[^devel]: The development version of the package is hosted on Github: https://github.com/dajmcdon/sparsegl.



We first create a small simulated dataset along with a vector indicating the grouping structure.

```{r, data-simulation}
set.seed(1010)
n <- 100
p <- 200
X <- matrix(data = rnorm(n*p), nrow = n, ncol = p)
beta <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), 
          c(2, -3, 8, 0, 0), rep(0, (p - 20)))
groups <- rep(1:(p / 5), each = 5)
eps <- rnorm(n, mean = 0, sd = 1)
y <- X %*% beta + eps
pr <- 1 / (1 + exp(-X %*% beta))
y0 <- rbinom(n, 1, pr)
```

The \pkg{sparsegl} package is mainly used with calls to 2 functions:  

* `sparsegl()`: fits sparse group regularized regression and classification models;
* `cv.sparsegl()`: repeatedly calls `sparsegl()` for the purposes of tuning parameter selection via cross validation.

The interface is intended to closely mimic that available in other \proglang{R} packages for regularized linear models, most notably \pkg{glmnet} \citep{R-glmnet}.
To perform the regularization path fitting at a sequence of regularization parameters, `sparsegl()` takes as required inputs, only `x`, the design matrix, and `y`, the response vector. Other optional arguments are the grouping vector `group`, the `family` (either `"gaussian"` or `"binomial"`), a penalty vector for group weights other than the size, the relative weight of lasso penalty, desired lower or upper bounds for coefficient estimates, and other optional configurations.

<!--
\textbf{sparsegl} object and its associated attributes. One of the main attribute of this object, \textbf{df}, is a vector representing the number of nonzero coefficients at each $\lambda$ value and also an approximation to the exact degree-of-freedom.
-->


```{r, eval=TRUE, echo=FALSE}
library(sparsegl)
```

```{r}
fit <- sparsegl(X, y, group = groups)
```

We include a number of \pkg{S3} methods for the `sparsegl` typical for linear models: `plot()`, `coef()`, `predict()` and `print()`. The `plot()` function displays either the coefficients or the group norms on the $y$-axis against either $\{\lambda_s\}_{s=1}^S$ or the scaled penalty on the $x$-axis. The resulting figures are shown in Figure \ref{fig:coef-trace}.

```{r coef-trace, fig.width = 4, fig.height = 2, fig.cap="Coefficient trace plots produced with the \\texttt{plot()} method.", fig.show='hold', out.width="45%"}
plot(fit, y_axis = "coef", x_axis = "penalty", add_legend = FALSE)
plot(fit, y_axis = "group", x_axis = "lambda", add_legend = FALSE)
```

The `coef()` and `predict()` methods give the coefficients or predicted values given a matrix $\tilde{\mX}$ at the requested $\lambda$s, potentially allowing for $\lambda$ values different from those used at the fitting stage. 

```{r, message=FALSE}
coef(fit, s = c(0.02, 0.03))[c(1,3,25,29),] # display a few
predict(fit, newx = tail(X), s = fit$lambda[2:3]) 
print(fit)
```

The `cv.sparsegl()` function implements $K$-fold cross validation and has a similar signature to `sparsegl()`. It allows the user to choose the number of splits and the loss function for measuring prediction or classification accuracy on the held-out sets.
Here, the \pkg{S3} `plot()` method displays the cross-validation curve with upper and lower confidence bounds plots calculated as $\pm 1$ standard error across the folds for each $\lambda$ in the regularization path. 

```{r cv-plot, fig.width = 4, fig.height = 2, fig.cap = "Cross validation error produced by the \\texttt{plot()} method for a \texttt{cv.sparsegl} object."}
cv_fit <- cv.sparsegl(X, y, groups, nfolds = 15)
plot(cv_fit)
```

The `coef()` and `predict()` methods work similarly to those above. The only differences are (1) that they can additionally accept the strings `lambda.min` or `lambda.1se`, respectively the $\lambda$ that minimizes the average cross validation error and the largest $\lambda$ such that the prediction error is within 1 standard error or the minimum.

```{r}
coef(cv_fit, s = "lambda.1se")[c(1,3,25,29),]
predict(cv_fit, newx = tail(X), s = "lambda.min")
```

For logistic regression, only a different `family` is required. Cross validation
can be implemented with misclassification or deviance loss.

```{r logitres, fig.width = 4, fig.height = 2, fig.cap="Cross validation error for logistic regression produced by the \\texttt{plot()} method using misclassification error on the held-out set."}
fit_logit <- sparsegl(X, y0, groups, family = "binomial")
cv_fit_logit <- cv.sparsegl(X, y0, groups, family = "binomial", 
                            pred.loss = "misclass")
plot(cv_fit_logit, log_axis = "none")
```

In some cases, when computations are at a premium, cross validation my be too demanding for the purposes of risk estimation. For this reason, \pkg{sparsegl} provides a `estimate_risk()` function. It can be used to compute AIC \citep{Akaike1973}, BIC \citep{Schwarz1978}, and GCV \citep{golub1979generalized}information criteria. All three are computed by combining the
log-likelihood with a penalty term for model flexibility. In addition to a fitted `sparsegl` model, `estimate_risk()` also needs the original data. The three available penalties are

- `"AIC"` (Akaike information criterion): $2\textrm{df} / n$;
- `"BIC"` (Bayesian information criterion): $\log(n)\textrm{df} / n$;
- `"GCV"` (Generalized cross validation): $-2\log(1 - \textrm{df} / n)$;  

where df is the degree-of-freedom, and $n$ is the sample size. Because the exact degrees-of-freedom can be computationally expensive, setting `approx_df=TRUE` uses the number of non-zero coefficient estimates, which is a fairly accurate approximation.

```{r risk-estimate, fig.width = 4, fig.height = 2}
er <- estimate_risk(fit, X, y)
```

```{r re-plot, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 2, fig.cap="AIC, BIC, and GCV (solid lines) along with their minima (vertical dashed lines)."}
library(tidyverse)
er <- er %>% select(-df) %>% pivot_longer(-lambda, values_to = "risk")
err <- er %>% group_by(name) %>% summarise(lambda = lambda[which.min(risk)])
ggplot(er, aes(lambda, risk, color = name)) +
  geom_line() +
  scale_color_brewer(palette = "Dark2") +
  geom_vline(data = err, aes(xintercept = lambda, color=name),
             linetype="dashed", show.legend = FALSE) +
  theme_bw() +
  ylab("Estimated risk") +
  xlab("Lambda") +
  scale_x_log10() +
  theme(legend.title = element_blank())
```

In this simulation, the $\lambda$ that minimizes AIC is `r round(err$lambda[1],3)` while the CV minimizer is `r round(cv_fit$lambda.min,3)`.

Additional documentation and examples are provided on the package [website](https://dajmcdon.github.io/sparsegl).

# Applications

## Geographic distribution of trust in experts {#sec:trust}

Two typical uses for sparse group lasso are (1) additive models where continuous predictors are expanded in a basis and (2) discrete factors as predictors. Here we demonstrate an example using both at the same time. We examine data from The Delphi Group at Carnegie Mellon University U.S. [COVID-19 Trends and Impact Survey (CTIS)](https://cmu-delphi.github.io/delphi-epidata/symptom-survey/contingency-tables.html), in partnership with Facebook. In particular, we examine the publicly available contingency table reports, which breakdown survey responses by age, race/ethnicity, gender, and other demographic variables of interest. The necessary data to reproduce this analysis (a sparse `dgCmatrix` object) is included in \pkg{sparsegl} as `trust_experts`.

In particular, we will focus on the "estimated percentage of respondents who trust ... to provide accurate news and information about COVID-19."  This survey item is reported for a variety of different potential sources of information---personal doctors/nurses, scientists, the Centers for Disease Control (CDC), government health officials, politicians, journalists, friends, and religious leaders. In this analysis, we average the first 4, characterize the combination as "experts", and use this as the response variable in a linear model.

We regress "trust in experts" on discrete predictors representing (1) month of report, (2) state of residence, (3) age group, (4) race/ethnicity, and (5) self-reported gender identity. This leads to 76 0-1 valued predictors in 5 groups. We also use spline basis expansions with 10 degrees-of-freedom to include continuous variables for "estimated percentage of people with Covid-like illness" and "estimated percentage of people reporting Covid-like illness in their local community, including their household" to control for the amount of exposure that respondents may have been having to the pandemic. 

After omitting both structural and otherwise missing data, the final model is estimated with 3775 observations on 96 predictors. Encoded as a sparse matrix, this takes requires about 500 KB of RAM to store, as opposed to 3 MB if it were dense. We estimated the model using `sparsegl()` and default arguments. Finally, we chose $\lambda$ to minimize BIC. Figure \ref{fig:trust} displays the estimated coefficients for the state-of-residence terms. Even controlling for age, race, gender, and the amount of circulating Covid-like illness, the United States displays strong geographic disparities when it comes to citizens' trust in scientists and other health authorities. 

```{r trust, echo=FALSE, fig.cap="State-level estimates for the amount of trust in experts about Covid-19.", fig.width=8, fig.height=4, message=FALSE, warning=FALSE, out.width="5in"}
source("code/covid.R")
g
```

## Estimating white matter connectivity

```{r brains, echo=FALSE, eval=TRUE, fig.cap="The group norm of the 61 groups based on neuroanatomical structure against the magnitude of the group penalty. The fit was produced using \\texttt{sparsegl()} to estimate the group lasso ($\\alpha=0$)."}
# source("code/brains.R") takes 4 GB memory and about 4 minutes
meta <- readRDS("large-data/brain-meta.rds")
fit <- readRDS("large-data/brain-fit.rds")
plot(fit, y_axis = "group", x_axis = "penalty", add_legend = FALSE)
```

Here we undertake an analysis similar to that in \cite{aminmansour2019} though with a slightly different dataset. The problem...

In \cite{aminmansour2019}, the authors propose an algorithm based on Orthogonal Matching Pursuit to estimate the group-regularized linear model. The data they use measures 12K voxels on 96 angles and attempts to reconstruct the white matter using the ENCODE method \citep{encode2017} with 1057 orientations and 868 fascicles. Converting this into a linear regression problem (rather than a tensor regression problem) results in a linear model with $n \approx 10^6$ and $p = 868$. 

In comparison, we use 150K voxels measured on `r meta$nangles` angles with 130K orientations and 26K streamlines. The resulting linear model has $n\approx 9M$ and $p=26K$, about 230$\times$ the size of the previous analysis. The design matrix would occupy nearly 2 PB if it were dense, but since it is only about 0.05% non-zero, it requires 1.2 GB of memory when stored in CSC format.
Estimating the group lasso using \pkg{sparsegl} with 61 groups based on neuroanatomy and 100 values of $\lambda$ took just under 2 minutes on a 2020 MacBook Air (Apple M1 chip) with 16 GB of RAM. Figure \ref{fig:brains} shows the group norm of the 61 groups against the magnitude of the group penalty.


# Discussion and future work

We developed this package for solving sparse group lasso optimization problem based on \textbf{four-step algorithm} with a convex loss function and a combination of $l1$ and $l2$ penalties. This preserves the grouping structure for coefficients by producing both among- and within- group sparsity, and also makes a huge improvement on computational efficiency compared to other existing sparse group lasso packages, which is achieved by applying strong rule check and KKT check iteratively.


# Acknowledgements {.unnumbered}

Here we list all the packages that we needed to compile this code. \pkg{devtools} [@R-devtools] \pkg{usethis} [@R-usethis] \pkg{testthat} [@R-testthat] \pkg{knitr} [@R-knitr] \pkg{ggplot2}[-@R-ggplot2] and \pkg{rticles} [-@R-rticles]. 

Daniel J. McDonald was partially supported by the National Science Foundation (DMS–1753171) and the National Sciences and Engineering Research Council of Canada (RGPIN-2021-02618). The analysis in Section \ref{sec:trust} is based on survey results from Carnegie Mellon University’s Delphi Group.


