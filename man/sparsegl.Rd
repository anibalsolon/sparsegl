% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sparsegl.R
\name{sparsegl}
\alias{sparsegl}
\title{Fits the regularization paths for sparse group-lasso penalized learning problems.}
\usage{
sparsegl(
  x,
  y,
  group = NULL,
  nlambda = 100,
  lambda.factor = ifelse(nobs < nvars, 0.01, 1e-04),
  lambda = NULL,
  pf = sqrt(bs),
  intercept = TRUE,
  asparse = 0.05,
  standardize = TRUE,
  lower_bnd = -Inf,
  upper_bnd = Inf,
  dfmax = as.integer(max(group)) + 1L,
  pmax = min(dfmax * 1.2, as.integer(max(group))),
  eps = 1e-08,
  maxit = 3e+08
)
}
\arguments{
\item{x}{Matrix of predictors, of dimension \eqn{n \times p}{n*p}; each row
is a vector of measurements and each column is a feature}

\item{y}{Real-valued response variable.}

\item{group}{A vector of consecutive integers describing the grouping of the
coefficients (see example below).}

\item{nlambda}{The number of \code{lambda} values - default is 100.}

\item{lambda.factor}{The factor for getting the minimal lambda in
\code{lambda} sequence, where \code{min(lambda)} = \code{lambda.factor} *
\code{max(lambda)}.  \code{max(lambda)} is the smallest value of
\code{lambda} for which all coefficients are zero. The default depends on
the relationship between \eqn{n} (the number of rows in the matrix of
predictors) and \eqn{p} (the number of predictors). If \eqn{n \geq p}, the
default is \code{0.001}, close to zero.  If \eqn{n<p}, the default is
\code{0.05}.  A very small value of \code{lambda.factor} will lead to a
saturated fit. It takes no effect if there is user-defined \code{lambda}
sequence.}

\item{lambda}{A user supplied \code{lambda} sequence. Typically, by leaving
this option unspecified users can have the program compute its own
\code{lambda} sequence based on \code{nlambda} and \code{lambda.factor}.
Supplying a value of \code{lambda} overrides this. It is better to supply a
decreasing sequence of \code{lambda} values than a single (small) value, if
not, the program will sort user-defined \code{lambda} sequence in decreasing
order automatically.}

\item{pf}{Penalty factor, a vector in length of bn (bn is the number
of groups in total). Separate penalty weights can be applied to each group of
\eqn{\beta}{beta's}s to allow differential shrinkage. Can be 0 for some
groups, which implies no shrinkage, and results in that group always being
included in the model. Default value for each entry is the square-root of
the corresponding size of each group.}

\item{intercept}{Whether to include intercept in the model. Default is TRUE.}

\item{asparse}{The weight to put on the ell1 norm in sparse group lasso. Default
is 0.05.}

\item{standardize}{Logical flag for variable standardization (scaling) prior
to fitting the model. Default is TRUE.}

\item{lower_bnd}{Lower bound for coefficient values, a vector in length of 1
or the number of groups including non-positive numbers only. Default value for
each entry is \code{-Inf}.}

\item{upper_bnd}{Upper bound for coefficient values, a vector in length of 1
or the number of groups including non-negative numbers only. Default value for
each entry is \code{Inf}.}

\item{dfmax}{Limit the maximum number of groups in the model. Useful for
very large \code{bs} (group size), if a partial path is desired. Default is
\code{bs+1}.}

\item{pmax}{Limit the maximum number of groups ever to be nonzero. For
example once a group enters the model, no matter how many times it exits or
re-enters model through the path, it will be counted only once. Default is
\code{min(dfmax*1.2,bs)}.}

\item{eps}{Convergence termination tolerance. Defaults value is \code{1e-8}.}

\item{maxit}{Maximum number of outer-loop iterations allowed at fixed lambda
value. Default is 3e8. If models do not converge, consider increasing
\code{maxit}.}
}
\value{
An object with S3 class \code{\link{sparsegl}}.  \item{call}{The call
that produced this object.} \item{b0}{Intercept sequence of length
\code{length(lambda)}.} \item{beta}{A \eqn{p\times \text{length}(\lambda)} matrix of
coefficients.} \item{df}{The number of nonzero features with nonzero coefficients
for each value of \code{lambda}.} \item{dim}{Dimension of coefficient matrix (ices).}
\item{lambda}{The actual sequence of \code{lambda} values used.}
\item{npasses}{Total number of iterations (the most inner loop) summed over
all lambda values.} \item{jerr}{Error flag, for warnings and errors, 0 if no
error.} \item{group}{A vector of consecutive integers describing the
grouping of the coefficients.}
}
\description{
Fits regularization paths for sparse group-lasso penalized learning problems at a
sequence of regularization parameters lambda.
}
\details{
Note that the objective function for \code{"ls"} least squares is
\deqn{RSS/(2n) + \lambda \times \text{penalty}}
Users can also tweak the penalty by choosing different penalty factor.

For computing speed reason, if models are not converging or running slow,
consider increasing \code{eps}, decreasing \code{nlambda}, or increasing
\code{lambda.factor} before increasing \code{maxit}.
}
\examples{
n <- 100
p <- 20
X <- matrix(rnorm(n * p), nrow = n)
eps <- rnorm(n)
beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), rep(0, (p - 15)))
y <- X \%*\% beta_star + eps
groups <- rep(1:(p / 5), each = 5)
fit1 <- sparsegl(X, y, group = groups)
}
\seealso{
\code{plot.sparsegl}, \code{coef.sparsegl}, \code{predict.sparsegl}
and \code{print.sparsegl} methods.
}
