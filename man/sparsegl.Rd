% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sparsegl.R
\name{sparsegl}
\alias{sparsegl}
\title{Fits the regularization paths for group-lasso penalized learning problems}
\usage{
sparsegl(
  x,
  y,
  group = NULL,
  nlambda = 100,
  lambda.factor = ifelse(nobs < nvars, 0.01, 1e-04),
  lambda = NULL,
  pf = sqrt(bs),
  intercept = TRUE,
  asparse = 0.05,
  standardize = TRUE,
  lower_bnd = -Inf,
  upper_bnd = Inf,
  dfmax = as.integer(max(group)) + 1L,
  pmax = min(dfmax * 1.2, as.integer(max(group))),
  eps = 1e-08,
  maxit = 3e+08
)
}
\arguments{
\item{x}{matrix of predictors, of dimension \eqn{n \times p}{n*p}; each row
is an observation vector.}

\item{y}{response variable. This argument should be quantitative for
regression (least squares), and a two-level factor for classification
(logistic model, huberized SVM, squared SVM).}

\item{group}{a vector of consecutive integers describing the grouping of the
coefficients (see example below).}

\item{nlambda}{the number of \code{lambda} values - default is 100.}

\item{lambda.factor}{the factor for getting the minimal lambda in
\code{lambda} sequence, where \code{min(lambda)} = \code{lambda.factor} *
\code{max(lambda)}.  \code{max(lambda)} is the smallest value of
\code{lambda} for which all coefficients are zero. The default depends on
the relationship between \eqn{n} (the number of rows in the matrix of
predictors) and \eqn{p} (the number of predictors). If \eqn{n >= p}, the
default is \code{0.001}, close to zero.  If \eqn{n<p}, the default is
\code{0.05}.  A very small value of \code{lambda.factor} will lead to a
saturated fit. It takes no effect if there is user-defined \code{lambda}
sequence.}

\item{lambda}{a user supplied \code{lambda} sequence. Typically, by leaving
this option unspecified users can have the program compute its own
\code{lambda} sequence based on \code{nlambda} and \code{lambda.factor}.
Supplying a value of \code{lambda} overrides this. It is better to supply a
decreasing sequence of \code{lambda} values than a single (small) value, if
not, the program will sort user-defined \code{lambda} sequence in decreasing
order automatically.}

\item{pf}{penalty factor, a vector in length of bn (bn is the total number
of groups). Separate penalty weights can be applied to each group of
\eqn{\beta}{beta's}s to allow differential shrinkage. Can be 0 for some
groups, which implies no shrinkage, and results in that group always being
included in the model. Default value for each entry is the square-root of
the corresponding size of each group.}

\item{intercept}{Whether to include intercept in the model. Default is TRUE.}

\item{asparse}{the weight to put on the ell1 norm in sparse group lasso. Default
is 0.05}

\item{dfmax}{limit the maximum number of groups in the model. Useful for
very large \code{bs} (group size), if a partial path is desired. Default is
\code{bs+1}.}

\item{pmax}{limit the maximum number of groups ever to be nonzero. For
example once a group enters the model, no matter how many times it exits or
re-enters model through the path, it will be counted only once. Default is
\code{min(dfmax*1.2,bs)}.}

\item{eps}{convergence termination tolerance. Defaults value is \code{1e-8}.}

\item{maxit}{maximum number of outer-loop iterations allowed at fixed lambda
value. Default is 3e8. If models do not converge, consider increasing
\code{maxit}.}

\item{pen}{a character string specifying the penalty function to use, valid
options are: \itemize{ \item \code{"sparsegl"} for group plus l1 penalty,
\item \code{"gglasso"} for group penalty only }Default is \code{"sparsegl"}.}

\item{algorithm}{a character string specifying the algorithm for sparse
group lasso (sparsegl). Valid options are: \itemize{ }}
}
\value{
An object with S3 class \code{\link{gglasso}}.  \item{call}{the call
that produced this object} \item{b0}{intercept sequence of length
\code{length(lambda)}} \item{beta}{a \code{p*length(lambda)} matrix of
coefficients.} \item{df}{the number of nonzero groups for each value of
\code{lambda}.} \item{dim}{dimension of coefficient matrix (ices)}
\item{lambda}{the actual sequence of \code{lambda} values used}
\item{npasses}{total number of iterations (the most inner loop) summed over
all lambda values} \item{jerr}{error flag, for warnings and errors, 0 if no
error.} \item{group}{a vector of consecutive integers describing the
grouping of the coefficients.}
}
\description{
Fits regularization paths for group-lasso penalized learning problems at a
sequence of regularization parameters lambda.
}
\details{
Note that the objective function for \code{"ls"} least squares is
\deqn{RSS/(2*n) + lambda * penalty;} for \code{"hsvm"} Huberized squared
hinge loss, \code{"sqsvm"} Squared hinge loss and \code{"logit"} logistic
regression, the objective function is \deqn{-loglik/n + lambda * penalty.}
Users can also tweak the penalty by choosing different penalty factor.

For computing speed reason, if models are not converging or running slow,
consider increasing \code{eps}, decreasing \code{nlambda}, or increasing
\code{lambda.factor} before increasing \code{maxit}.
}
\examples{
# load sparsegl library
library(sparsegl)

# define group index
group1 <- rep(1:20,each=5)

# fit group lasso penalized least squares
# m1 <- sparsegl(x=bardet$x,y=bardet$y,group=group1,pen="gglasso")
# define group index
group2 <- rep(1:20,each=5)


}
\seealso{
\code{plot.gglasso}
}
\author{
Aaron Cohen and Dan McDonald\cr Maintainer: Aaron Cohen \href{mailto:cohenaa@indiana.edu}{cohenaa@indiana.edu}
}
\keyword{models}
\keyword{regression}
\keyword{sparse}
